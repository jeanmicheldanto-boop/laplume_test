#!/usr/bin/env python3
"""
Pipeline de pseudonymisation hybride : NER chunked + R√®gles avec r√©solution de conflits
"""
import argparse
import csv
import json
import logging
import re
import sys
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Set
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline, CamembertTokenizer

# Import des nouveaux modules d'am√©lioration
try:
    from text_processing import TextNormalizer, StopWordsFilter, PriorityMatrix
    ENHANCED_MODULES_AVAILABLE = True
except ImportError:
    # Fallback si les modules ne sont pas disponibles
    TextNormalizer = None
    StopWordsFilter = None
    PriorityMatrix = None
    ENHANCED_MODULES_AVAILABLE = False


def _norm_key(s: str) -> str:
    """Normalise une cl√© pour l'indexation en supprimant les espaces multiples et normalisant les apostrophes"""
    return re.sub(r"\s+", " ", s.replace("'", "'")).strip()


def _is_establishment_name(text: str, context: str) -> bool:
    """D√©termine si un nom ressemble √† un √©tablissement bas√© sur des heuristiques"""
    # Normaliser le texte
    norm_text = _norm_key(text)
    
    # Heuristique 1: Commence par des articles caract√©ristiques d'√©tablissements
    if norm_text.startswith(("Les ", "Le ", "La ", "L'")):
        # Exclure les √©coles g√©n√©rales et les villes connues
        if (not norm_text.lower().startswith(("√©cole", "coll√®ge", "lyc√©e")) and
            not any(city in norm_text.lower() for city in [
                "paris", "lyon", "marseille", "toulouse", "nice", "nantes", 
                "montpellier", "strasbourg", "bordeaux", "lille", "rennes", 
                "reims", "nanterre", "bobigny", "cr√©teil", "montreuil"
            ])):
            return True
    
    # Heuristique 2: Contexte contient des verbes indicateurs d'√©tablissement
    establishment_verbs = [
        "accueille", "accompagne", "h√©berge", "prend en charge", "suit", 
        "oriente", "√©value", "propose", "offre", "dispense", "assure"
    ]
    if any(verb in context for verb in establishment_verbs):
        return True
    
    # Heuristique 3: Noms d'arbres/plantes (souvent utilis√©s pour les √©tablissements)
    plant_names = [
        "tilleuls", "peupliers", "ch√™nes", "ormes", "platanes", "√©rables",
        "acacias", "cypr√®s", "pins", "sapins", "bouleaux", "fr√™nes",
        "roses", "lilas", "jasmin", "glycines", "magnolias"
    ]
    if any(plant in norm_text.lower() for plant in plant_names):
        return True
    
    return False


class PseudonymStore:
    """Gestionnaire de pseudonymisation avec compteurs et sauvegarde"""
    
    def __init__(self):
        self.forward = {}  # original -> pseudonyme
        self.reverse = {}  # pseudonyme -> original
        self.counters = {"PER": 0, "ORG": 0, "LOC": 0, "ETAB": 0, "DATE": 0, "PHONE": 0, "NIR": 0, "TIME": 0, "EMAIL": 0}
    
    def pseudonymize(self, text: str, label: str, category: str = None) -> str:
        """Pseudonymise un texte ou retourne le pseudonyme existant"""
        if text in self.forward:
            return self.forward[text]
        
        # Utiliser la cat√©gorie sp√©cifique si fournie, sinon utiliser le label g√©n√©rique
        pseudo_label = category if category else label
        
        # Cr√©er le compteur pour cette cat√©gorie si n√©cessaire
        if pseudo_label not in self.counters:
            self.counters[pseudo_label] = 0
            
        self.counters[pseudo_label] += 1
        replacement = f"<{pseudo_label}_{self.counters[pseudo_label]}>"
        self.forward[text] = replacement
        self.reverse[replacement] = text
        return replacement
    
    def save(self, path: str):
        """Sauvegarde le mapping au format JSON"""
        data = {
            "forward": self.forward,
            "reverse": self.reverse,
            "counters": self.counters,
            "stats": {
                "total_entities": len(self.forward),
                "per_type": dict(self.counters)
            }
        }
        
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logging.info(f"üíæ Mapping sauv√©: {path}")
    
    @classmethod
    def load(cls, path: str) -> "PseudonymStore":
        """Charge un mapping existant"""
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        store = cls()
        store.forward = data.get("forward", {})
        store.reverse = data.get("reverse", {})
        store.counters = data.get("counters", {"PER": 0, "ORG": 0, "LOC": 0, "ETAB": 0})
        
        logging.info(f"üìÇ Mapping charg√©: {path}")
        return store
    
    def depseudonymize(self, text: str) -> str:
        """Restaure le texte original √† partir des pseudonymes"""
        result = text
        for pseudonym, original in self.reverse.items():
            result = result.replace(pseudonym, original)
        return result


class GazetteerEngine:
    """Moteur de gazetteers pour d√©tecter les entit√©s par correspondance exacte"""
    
    def __init__(self, gazetteer_dir: str):
        self.gazetteer_dir = Path(gazetteer_dir)
        self.gazetteers = {}
        self._load_gazetteers()
    
    def _load_gazetteers(self):
        """Charge tous les fichiers CSV de gazetteers"""
        if not self.gazetteer_dir.exists():
            logging.warning(f"‚ö†Ô∏è Dossier gazetteers introuvable: {self.gazetteer_dir}")
            return
        
        csv_files = list(self.gazetteer_dir.glob("*.csv"))
        if not csv_files:
            logging.warning(f"‚ö†Ô∏è Aucun fichier CSV trouv√© dans: {self.gazetteer_dir}")
            return
        
        for csv_file in csv_files:
            gazetteer_name = csv_file.stem
            entries = []
            
            try:
                with open(csv_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        if 'name' in row and 'category' in row:
                            entries.append({
                                'name': row['name'].strip(),
                                'category': row['category'].strip()
                            })
                        elif 'keyword' in row and 'category' in row:
                            entries.append({
                                'name': row['keyword'].strip(),
                                'category': row['category'].strip()
                            })
                
                if entries:
                    self.gazetteers[gazetteer_name] = entries
                    logging.info(f"üìö Gazetteer {gazetteer_name}: {len(entries)} entr√©es")
                
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Erreur chargement {csv_file}: {e}")
        
        total_entries = sum(len(entries) for entries in self.gazetteers.values())
        logging.info(f"‚úÖ Gazetteers charg√©s: {len(self.gazetteers)} fichiers, {total_entries} entr√©es")
    
    def detect_entities(self, text: str) -> List[Dict]:
        """D√©tecte les entit√©s selon les gazetteers par correspondance exacte"""
        entities = []
        exclusions_count = 0
        
        for gazetteer_name, entries in self.gazetteers.items():
            # Traiter gazetteer_exclusions s√©par√©ment (ne pas cr√©er d'entit√©s)
            if gazetteer_name == 'gazetteer_exclusions':
                continue
                
            for entry in entries:
                name = entry['name']
                category = entry['category']
                
                # Recherche case-insensitive avec correspondance de mots entiers
                pattern = r'\b' + re.escape(name) + r'\b'
                
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    # Trim le texte d√©tect√©
                    matched_text = match.group().strip()
                    
                    # Ignorer si vide apr√®s trim ou trop court (< 2 chars sauf codes)
                    if not matched_text or (len(matched_text) < 2 and not category.endswith('_CP')):
                        continue
                    
                    # D√©terminer le label final
                    if category.startswith('ORG_'):
                        label = 'ORG'
                    elif category.startswith('ETAB_'):
                        label = 'ETAB'
                    else:
                        label = 'ORG'  # Par d√©faut
                    
                    entities.append({
                        "start": match.start(),
                        "end": match.end(),
                        "text": matched_text,  # Utiliser le texte trim√©
                        "label": label,
                        "source": "GAZETTEER",
                        "category": category,
                        "gazetteer": gazetteer_name,
                        "score": 1.0  # Score maximum pour gazetteers
                    })
        
        logging.info(f"üìñ Gazetteers d√©tect√©s: {len(entities)} entit√©s")
        return entities


class RulesEngine:
    """Moteur de r√®gles pour d√©tecter les organisations, √©tablissements et donn√©es sensibles"""
    
    def __init__(self, rules_file: str):
        self.rules_file = rules_file
        self.org_patterns = {}
        self.etab_patterns = {}
        self.date_patterns = {}
        self.phone_patterns = {}
        self.nir_patterns = {}
        self.time_patterns = {}
        self.email_patterns = {}
        self.profession_patterns = {}
        self._load_rules()
    
    def _load_rules(self):
        """Charge les r√®gles depuis le fichier YAML"""
        if not Path(self.rules_file).exists():
            logging.warning(f"‚ö†Ô∏è Fichier de r√®gles introuvable: {self.rules_file}")
            return
        
        with open(self.rules_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # R√®gles ORG
        org_rules = data.get('org_regex', {})
        for category, patterns in org_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.org_patterns[category] = compiled_patterns
        
        # R√®gles ETAB
        etab_rules = data.get('etab_categories', {})
        for category, patterns in etab_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.etab_patterns[category] = compiled_patterns
        
        # R√®gles DATE
        date_rules = data.get('date_regex', {})
        for category, patterns in date_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.date_patterns[category] = compiled_patterns
        
        # R√®gles PHONE
        phone_rules = data.get('phone_regex', {})
        for category, patterns in phone_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.phone_patterns[category] = compiled_patterns
        
        # R√®gles NIR
        nir_rules = data.get('nir_regex', {})
        for category, patterns in nir_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.nir_patterns[category] = compiled_patterns
        
        # R√®gles TIME
        time_rules = data.get('time_regex', {})
        for category, patterns in time_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.time_patterns[category] = compiled_patterns
        
        # R√®gles EMAIL
        email_rules = data.get('email_regex', {})
        for category, patterns in email_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.email_patterns[category] = compiled_patterns
        
        # NOTE: Professions supprim√©es - ne pas pseudonymiser les professions (risque de r√©-identification)
        
        total_patterns = (len(self.org_patterns) + len(self.etab_patterns) + len(self.date_patterns) + 
                         len(self.phone_patterns) + len(self.nir_patterns) + len(self.time_patterns) + 
                         len(self.email_patterns))
        logging.info(f"‚úÖ R√®gles charg√©es: {len(self.org_patterns)} ORG, {len(self.etab_patterns)} ETAB, "
                    f"{len(self.date_patterns)} DATE, {len(self.phone_patterns)} PHONE, "
                    f"{len(self.nir_patterns)} NIR, {len(self.time_patterns)} TIME, "
                    f"{len(self.email_patterns)} EMAIL")
    
    def detect_entities(self, text: str) -> List[Dict]:
        """D√©tecte les entit√©s selon les r√®gles avec gestion des entit√©s compos√©es"""
        entities = []
        
        # Patterns de localisation pour d√©tecter les entit√©s compos√©es
        loc_patterns = [
            r"\b(√†|au|aux|dans|sur|de|du|des)\s+[A-Z√â√à√Ä√Ç√é√ô√î√á][A-Za-z√Ä-√ñ√ò-√∂√∏-√ø''\-\s]+\b",
            r"\bde\s+[A-Z√â√à√Ä√Ç√é√ô√î√á][A-Za-z√Ä-√ñ√ò-√∂√∏-√ø''\-\s]+\b"
        ]
        
        # D√©tecter les ORG avec extension g√©ographique
        for category, patterns in self.org_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    start, end = match.start(), match.end()
                    base_text = match.group().strip()  # Trim le texte de base
                    
                    if not base_text:  # Ignorer si vide apr√®s trim
                        continue
                    
                    # Ajuster les positions apr√®s trim
                    left_strip = len(match.group()) - len(match.group().lstrip())
                    start = start + left_strip
                    end = start + len(base_text)
                    
                    # Chercher une extension g√©ographique apr√®s l'entit√©
                    extended_text = base_text
                    extended_end = end
                    
                    # Chercher "de/du/des + lieu" apr√®s l'entit√© de base
                    remaining_text = text[end:]
                    for loc_pattern in loc_patterns:
                        loc_match = re.match(r'\s*' + loc_pattern, remaining_text)
                        if loc_match:
                            extension = loc_match.group().strip()
                            extended_text = base_text + " " + extension
                            extended_end = end + loc_match.end()
                            break
                    
                    entities.append({
                        "start": start,
                        "end": extended_end,
                        "text": extended_text,
                        "label": "ORG",
                        "source": "RULES",
                        "category": category,  # Cat√©gorie sp√©cifique
                        "score": 1.0
                    })
        
        # D√©tecter les ETAB avec extension g√©ographique
        for category, patterns in self.etab_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    start, end = match.start(), match.end()
                    base_text = match.group().strip()  # Trim le texte de base
                    
                    if not base_text:  # Ignorer si vide apr√®s trim
                        continue
                    
                    # Ajuster les positions apr√®s trim
                    left_strip = len(match.group()) - len(match.group().lstrip())
                    start = start + left_strip
                    end = start + len(base_text)
                    
                    # Chercher une extension g√©ographique apr√®s l'entit√©
                    extended_text = base_text
                    extended_end = end
                    
                    # Chercher "de/du/des + lieu" apr√®s l'entit√© de base
                    remaining_text = text[end:]
                    for loc_pattern in loc_patterns:
                        loc_match = re.match(r'\s*' + loc_pattern, remaining_text)
                        if loc_match:
                            extension = loc_match.group().strip()
                            extended_text = base_text + " " + extension
                            extended_end = end + loc_match.end()
                            break
                    
                    entities.append({
                        "start": start,
                        "end": extended_end,
                        "text": extended_text,
                        "label": "ETAB",
                        "source": "RULES",
                        "category": category,  # Cat√©gorie sp√©cifique
                        "score": 1.0
                    })
        
        # D√©tecter les autres types de donn√©es sensibles (sans extension g√©ographique)
        # NOTE: profession_patterns retir√© - ne pas pseudonymiser les professions
        sensitive_data_types = [
            ("date_patterns", "DATE"),
            ("phone_patterns", "PHONE"),
            ("nir_patterns", "NIR"),
            ("time_patterns", "TIME"),
            ("email_patterns", "EMAIL")
        ]
        
        for pattern_attr, label in sensitive_data_types:
            patterns_dict = getattr(self, pattern_attr, {})
            for category, patterns in patterns_dict.items():
                for pattern in patterns:
                    for match in pattern.finditer(text):
                        # Trim le texte d√©tect√©
                        matched_text = match.group().strip()
                        if not matched_text:  # Ignorer si vide apr√®s trim
                            continue
                        
                        # Calculer les vraies positions apr√®s trim
                        left_strip = len(match.group()) - len(match.group().lstrip())
                        start_trimmed = match.start() + left_strip
                        end_trimmed = start_trimmed + len(matched_text)
                        
                        entities.append({
                            "start": start_trimmed,
                            "end": end_trimmed,
                            "text": matched_text,
                            "label": label,
                            "source": "RULES",
                            "category": category,
                            "score": 1.0
                        })
        
        logging.info(f"üîß R√®gles d√©tect√©es: {len(entities)} entit√©s")
        return entities


class ChunkedNER:
    """D√©tecteur NER avec d√©coupage en chunks"""
    
    def __init__(self, model_name: str, max_tokens: int = 400):
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.tokenizer = None
        self.pipeline = None
        self._load_model()
    
    def _load_model(self):
        """Charge le mod√®le NER"""
        logging.info(f"ü§ñ Chargement mod√®le NER: {self.model_name}")
        
        try:
            # Essayer d'abord avec AutoTokenizer fast
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Tokenizer fast √©chou√©, utilisation CamembertTokenizer: {e}")
            # Fallback vers CamembertTokenizer explicite
            self.tokenizer = CamembertTokenizer.from_pretrained(self.model_name)
        
        model = AutoModelForTokenClassification.from_pretrained(self.model_name)
        self.pipeline = pipeline(
            "token-classification", 
            model=model, 
            tokenizer=self.tokenizer, 
            aggregation_strategy="simple"
        )
        
        logging.info("‚úÖ Mod√®le NER charg√©")
    
    def _split_into_chunks(self, text: str) -> List[Tuple[str, int, int]]:
        """D√©coupe le texte en chunks avec positions"""
        sentences = text.split('. ')
        chunks = []
        current_pos = 0
        
        i = 0
        while i < len(sentences):
            chunk_sentences = []
            chunk_tokens = 0
            
            while i < len(sentences) and chunk_tokens < self.max_tokens:
                sentence = sentences[i]
                sentence_tokens = len(self.tokenizer.encode(sentence, add_special_tokens=False))
                
                if chunk_tokens + sentence_tokens > self.max_tokens + 50 and chunk_sentences:
                    break
                
                chunk_sentences.append(sentence)
                chunk_tokens += sentence_tokens
                i += 1
            
            if not chunk_sentences:
                chunk_sentences = [sentences[i]]
                i += 1
            
            chunk_text = '. '.join(chunk_sentences)
            if not chunk_text.endswith('.'):
                chunk_text += '.'
            
            chunk_start = current_pos
            chunk_end = chunk_start + len(chunk_text)
            
            chunks.append((chunk_text, chunk_start, chunk_end))
            
            current_pos += len(chunk_text)
            if i < len(sentences):
                current_pos += 1
        
        logging.info(f"üìè Texte divis√© en {len(chunks)} chunks")
        return chunks

    def detect_entities(self, text: str) -> List[Dict]:
        """D√©tecte les entit√©s NER avec d√©coupage en chunks"""
        chunks = self._split_into_chunks(text)
        all_entities = []
        
        for chunk_idx, (chunk_text, chunk_start, chunk_end) in enumerate(chunks):
            logging.debug(f"üß© Traitement chunk {chunk_idx + 1}/{len(chunks)}")
            
            try:
                chunk_entities = self.pipeline(chunk_text)
                logging.debug(f"üîç Chunk {chunk_idx} NER r√©sultat brut: {chunk_entities}")
                
                for entity in chunk_entities:
                    label = entity.get("entity_group")
                    if label in {"PER", "ORG", "LOC"}:
                        if entity.get("start") is None or entity.get("end") is None:
                            logging.warning(f"‚ö†Ô∏è Entit√© avec start/end None: {entity}")
                            continue
                            
                        global_start = chunk_start + int(entity["start"])
                        global_end = chunk_start + int(entity["end"])
                        entity_text = text[global_start:global_end]
                        
                        # üîß Trim le texte et ajuster les positions
                        entity_text_trimmed = entity_text.strip()
                        if not entity_text_trimmed:  # Ignorer si vide apr√®s trim
                            continue
                        
                        # Calculer le d√©calage d√ª au trim
                        left_strip = len(entity_text) - len(entity_text.lstrip())
                        global_start_trimmed = global_start + left_strip
                        global_end_trimmed = global_start_trimmed + len(entity_text_trimmed)
                        
                        all_entities.append({
                            "start": global_start_trimmed,
                            "end": global_end_trimmed,
                            "text": entity_text_trimmed,
                            "label": label,
                            "source": "NER",
                            "score": float(entity.get("score", 0.0)),
                            "chunk": chunk_idx
                        })
                        
            except Exception as ex:
                logging.warning(f"‚ö†Ô∏è Erreur chunk {chunk_idx}: {ex}")
        
        logging.info(f"ü§ñ NER d√©tect√©: {len(all_entities)} entit√©s")
        return all_entities


class ConflictResolver:
    """R√©solveur de conflits entre NER, r√®gles et gazetteers avec syst√®me de priorit√©"""
    
    def __init__(self, exclusions: List[str] = None):
        # Liste d'exclusions (termes √† ne jamais pseudonymiser)
        self.exclusions = set(exclusions) if exclusions else set()
        
        # Priorit√©s : plus haut = prioritaire (matrice am√©lior√©e)
        self.priorities = {
            # Donn√©es sensibles critiques
            "RULES_EMAIL": 10.0,
            "RULES_PHONE": 9.5,
            "RULES_NIR": 9.4,
            "ENHANCED_ADDR_FULL": 9.3,
            "RULES_ADDR_FULL": 9.2,
            
            # Fusion TYPE+LOC - priorit√© TR√àS √©lev√©e (TYPE explicite prime sur gazetteer)
            "MERGED_TYPE_LOC_ETAB": 9.1,
            
            # Gazetteers - priorit√© absolue pour entit√©s connues
            "GAZETTEER_ORG": 9.0,
            "GAZETTEER_ETAB": 9.0,
            
            # Organisations sp√©cialis√©es (priorit√© √©lev√©e)
            "RULES_ORG_CHU_CH": 8.5,
            "ENHANCED_ORG_CHU_CH": 8.5,
            "RULES_ORG_JUSTICE": 8.4,
            "RULES_ORG_MDPH": 8.3,
            "RULES_ORG_ARS": 8.2,
            "RULES_ORG_DEPARTEMENT": 8.1,
            "RULES_ORG_PREFECTURE": 8.0,
            
            # Sources contextuelles (priorit√© √©lev√©e car tr√®s fiables)
            "CONTEXTUAL_PER": 7.9,
            "CONTEXTUAL_PROFESSION": 7.8,
            
            # √âtablissements sp√©cialis√©s
            "RULES_EHPAD": 7.9,
            "RULES_IME": 7.8,
            "RULES_ITEP": 7.7,
            "RULES_MECS": 7.6,
            "RULES_ESAT": 7.5,
            "RULES_PROFESSION": 7.0,  # Priorit√© √©lev√©e pour √©viter ORG
            
            # √âtablissements avec noms (ENHANCED)
            "ENHANCED_ETAB_MECS": 6.8,
            "ENHANCED_ETAB_IME": 6.7,
            "ENHANCED_ETAB_ITEP": 6.6,
            "ENHANCED_ETAB_SESSAD": 6.5,
            "ENHANCED_ETAB_ESAT": 6.4,
            "ENHANCED_ETAB_EHPAD": 6.3,
            "ENHANCED_ETAB_MAS": 6.2,
            "ENHANCED_ETAB_FAM": 6.1,
            "ENHANCED_ETAB_FOYER_VIE": 6.0,
            
            # Adresses sp√©cialis√©es
            "ENHANCED_ADDR_STREET": 5.5,
            "RULES_ADDR_STREET": 5.4,
            "ENHANCED_LOC_CITY": 5.0,
            
            # Temporel
            "RULES_DATE": 4.5,
            "RULES_TIME": 4.4,
            
            # Localisation (priorit√© plus basse)
            "NER_LOC_CITY": 3.5,
            "NER_LOC": 3.0,
            
            # NER et r√®gles g√©n√©riques
            "NER_PER": 2.5,
            "RULES_ORG": 2.0,
            "RULES_ETAB": 2.0,
            "NER_ORG": 1.5,
            "DEFAULT": 1.0
        }
        
        # Initialiser les modules d'am√©lioration si disponibles
        if ENHANCED_MODULES_AVAILABLE:
            self.text_normalizer = TextNormalizer()
            self.stopwords_filter = StopWordsFilter()
            self.priority_matrix = PriorityMatrix()
        else:
            self.text_normalizer = None
            self.stopwords_filter = None
            self.priority_matrix = None
    
    def _calculate_overlap(self, entity1: Dict, entity2: Dict) -> float:
        """Calcule le taux de chevauchement entre deux entit√©s"""
        start1, end1 = entity1["start"], entity1["end"]
        start2, end2 = entity2["start"], entity2["end"]
        
        overlap_start = max(start1, start2)
        overlap_end = min(end1, end2)
        
        if overlap_start >= overlap_end:
            return 0.0
        
        overlap_length = overlap_end - overlap_start
        min_length = min(end1 - start1, end2 - start2)
        
        return overlap_length / min_length if min_length > 0 else 0.0
    
    def _get_priority_key(self, entity: Dict) -> str:
        """G√©n√®re la cl√© de priorit√© pour une entit√©"""
        source = entity["source"]
        label = entity["label"]
        category = entity.get("category")
        
        # Fusion TYPE+LOC a une cl√© sp√©ciale pour priorit√© tr√®s haute
        if source == "MERGED_TYPE_LOC":
            return "MERGED_TYPE_LOC_ETAB"
        elif source == "GAZETTEER":
            return f"GAZETTEER_{label}"
        elif source == "RULES":
            # Utiliser la cat√©gorie sp√©cifique si disponible
            if category:
                return f"RULES_{category}"
            else:
                return f"RULES_{label}"
        else:
            return f"NER_{label}"
    
    def _get_priority(self, entity: Dict) -> float:
        """Retourne la priorit√© d'une entit√©"""
        priority_key = self._get_priority_key(entity)
        return self.priorities.get(priority_key, self._get_default_priority(entity))
    
    def _is_valid_entity(self, entity: Dict) -> bool:
        """Filtre les entit√©s invalides"""
        text = entity["text"].strip()
        
        # üö´ EXCLUSIONS : v√©rifier si le texte est dans la liste d'exclusion
        if text in self.exclusions or text.lower() in self.exclusions:
            logging.debug(f"üö´ Entit√© exclue (stoplist): '{text}'")
            return False
        
        # √âliminer les cha√Ænes vides ou trop courtes (sauf codes postaux, t√©l√©phones)
        if len(text) < 2:
            logging.debug(f"üóëÔ∏è Entit√© trop courte ignor√©e: '{text}'")
            return False
        
        # Filtrer les fragments d'adresse isol√©s
        address_fragments = ["du", "de la", "de l", "des", "Les", "P√¥le", "Industrie", "rue de l"]
        if text in address_fragments:
            logging.debug(f"üóëÔ∏è Fragment d'adresse ignor√©: '{text}'")
            return False
            
        # √âliminer les entit√©s qui sont juste des lettres isol√©es pour ORG
        if entity["label"] == "ORG" and len(text) <= 2 and text.isalpha():
            logging.debug(f"üóëÔ∏è ORG trop court ignor√©: '{text}'")
            return False
        
        # √âliminer les abr√©viations ambigu√´s sans contexte clair
        ambiguous_short = ["ME", "C", "M", "AS", "ES", "IS"]
        if text.strip() in ambiguous_short and entity["label"] in ["ORG", "ETAB"]:
            logging.debug(f"üóëÔ∏è Abr√©viation ambigu√´ ignor√©e: '{text}'")
            return False
            
        # √âliminer les intitul√©s de fonction/m√©tier 
        job_titles = [
            "technicien intervention sociale et familiale",
            "assistant familial", "√©ducateur", "psychologue",
            "travailleur social", "aide soignant", "conseiller",
            "r√©f√©rent", "coordinateur", "superviseur"
        ]
        
        if any(job in text.lower() for job in job_titles):
            logging.debug(f"üóëÔ∏è Intitul√© de fonction ignor√©: '{text}'")
            return False
        
        # √âliminer les doublons d'organisations (garder la version la plus sp√©cifique)
        if entity["label"] == "ORG" and entity.get("category") != "ORG_ENTREPRISE_PRIV":
            # Si c'est un nom g√©n√©rique d'organisation d√©j√† couvert par une r√®gle sp√©cialis√©e
            generic_orgs = ["conseil d√©partemental", "d√©partement", "prefecture"]
            if text.lower() in generic_orgs:
                logging.debug(f"üóëÔ∏è Organisation g√©n√©rique ignor√©e (r√®gle sp√©cialis√©e existe): '{text}'")
                return False
            
        return True
    
    def _enhance_entity_classification(self, entity: Dict, full_text: str) -> Dict:
        """Am√©liore la classification des entit√©s selon le contexte"""
        text = entity["text"]
        start = entity["start"]
        end = entity["end"]
        
        # R√©cup√©rer le contexte √©largi (¬±50 caract√®res pour √™tre plus pr√©cis)
        context_start = max(0, start - 50)
        context_end = min(len(full_text), end + 50)
        context = full_text[context_start:context_end].lower()
        
        # Patterns d'√©tablissements pour expansion PR√âCISE
        etab_patterns = {
            "ETAB_MECS": ["mecs", "maison d'enfants"],
            "ETAB_IME": ["ime"],
            "ETAB_ITEP": ["itep"],
            "ETAB_SESSAD": ["sessad"],
            "ETAB_ESAT": ["esat", "√©tablissement et service d'aide par le travail"],
            "ETAB_EHPAD": ["ehpad", "r√©sidence m√©dicalis√©e"],
            "ETAB_FOYER_VIE": ["foyer de vie"],
            "ETAB_MAS": ["mas", "maison d'accueil sp√©cialis√©e"],
            "ETAB_FAM": ["fam", "foyer d'accueil m√©dicalis√©"]
        }
        
        # Patterns pour identifier les adresses
        address_patterns = [
            r"\b(?:rue|avenue|boulevard|bd|impasse|place|pl|chemin|all√©e)\s+",
            r"\b\d{5}\b",  # Code postal
            r"\b\d{1,4}(?:bis|ter)?\s+(?:rue|avenue|boulevard|impasse)",
        ]
        
        # Patterns pour identifier les villes
        city_patterns = [
            r"\b(?:paris|marseille|lyon|toulouse|nice|nantes|montpellier|strasbourg|bordeaux|lille|rennes|reims|le havre|saint-√©tienne|toulon|grenoble|dijon|angers|n√Æmes|villeurbanne|saint-denis|le mans|aix-en-provence|clermont-ferrand|brest|limoges|tours|amiens|perpignan|metz|besan√ßon|boulogne-billancourt|orl√©ans|mulhouse|rouen|caen|nancy|saint-paul|argenteuil|montreuil|roubaix|tourcoing|nanterre|avignon|cr√©teil|dunkerque|poitiers|asni√®res-sur-seine|versailles|courbevoie|vitry-sur-seine|colombes|pau|aulnay-sous-bois|rueil-malmaison|saint-pierre|antibes|saint-maur-des-foss√©s|cannes|boulogne-sur-mer|noum√©a|calais|drancy|cergy|saint-nazaire|colmar|issy-les-moulineaux|noisy-le-grand|√©vry|villeneuve-d'ascq|la rochelle|antony|troyes|pessac|ivry-sur-seine|clichy|chamb√©ry|lorient|montauban|niort|s√®te|vincennes|saint-ouen|la seyne-sur-mer|villejuif|saint-andr√©|clichy-sous-bois|√©pinay-sur-seine|meaux|merignac|valence|saint-priest|noisy-le-sec|pantin|v√©nissieux|caluire-et-cuire|bourges|la courneuve|cholet|sartrouville|mantes-la-jolie|bobigny)\b",
            r"\b(?:paris\s+\d{1,2})\b"  # Arrondissements parisiens
        ]
        
        import re
        
        # Classification des adresses et entit√©s sp√©cialis√©es
        if entity["label"] in ["LOC", "ORG"] and entity["source"] == "NER":
            # D'abord v√©rifier si c'est un CHU/Centre Hospitalier avant de classer comme ville
            chu_patterns = [
                r"\bCHU\s+(?:de\s+)?[A-Z√â][\w''\-]+(?:\s+[A-Z√â][\w''\-]+){0,2}\b",
                r"\b(?:Centre\s+Hospitalier|CHR)\s+(?:de\s+)?[A-Z√â][\w''\-]+(?:\s+[A-Z√â][\w''\-]+){0,2}\b"
            ]
            
            for chu_pattern in chu_patterns:
                if re.search(chu_pattern, text, re.IGNORECASE):
                    entity["label"] = "ORG_CHU_CH"
                    entity["category"] = "ORG_CHU_CH"
                    entity["source"] = "ENHANCED"
                    logging.debug(f"üè• CHU detected: '{text}' ‚Üí ORG_CHU_CH")
                    return entity
            
            # Types d'√©tablissements qui n√©cessitent une fusion (√† v√©rifier AVANT reclassification)
            fusion_types = ['EHPAD', 'MAS', 'FAM', 'MECS', 'IME', 'ITEP', 'SESSAD',
                           'ESAT', 'CMPP', 'CAMSP', 'FJT', 'CHRS', 'IMPRO', 'IEM', 
                           'IES', 'SAFEP', 'SSEFS', 'EEAP',
                           'SAVS', 'SAMSAH', 'SSIAD']  # Noms courts pour d√©tection pr√©fixe
            
            # V√©rifier si un TYPE d'√©tablissement est imm√©diatement AVANT (gap ‚â§ 3 caract√®res)
            prefix_context = full_text[max(0, start - 15):start].strip().upper()
            has_adjacent_type = any(ftype in prefix_context for ftype in fusion_types)
            
            # V√©rifier si c'est une adresse (SAUF si type √©tablissement adjacent)
            is_address = any(re.search(pattern, text.lower()) for pattern in address_patterns)
            if is_address and not has_adjacent_type:
                if re.search(r"\b\d{5}\b", text):
                    entity["label"] = "ADDR_FULL"
                    entity["category"] = "ADDR_FULL"
                else:
                    entity["label"] = "ADDR_STREET"
                    entity["category"] = "ADDR_STREET"
                entity["source"] = "ENHANCED"
                logging.debug(f"üè† Address detected: '{text}' ‚Üí {entity['label']}")
                return entity
            
            # V√©rifier si c'est une ville (SAUF si type √©tablissement adjacent)
            is_city = any(re.search(pattern, text.lower()) for pattern in city_patterns)
            if is_city and not has_adjacent_type:
                entity["label"] = "LOC_CITY"
                entity["category"] = "LOC_CITY"
                entity["source"] = "ENHANCED"
                logging.debug(f"üåç City detected: '{text}' ‚Üí LOC_CITY")
                return entity
            
            # Si type √©tablissement adjacent, pr√©server LOC pour fusion
            if has_adjacent_type:
                logging.debug(f"üîó LOC '{text}' pr√©serv√© pour fusion avec TYPE adjacent (√©vite reclassification ville/adresse)")
        
        # D√©tecter si le NER a captur√© TYPE+NOM ensemble (ex: "SESSAD Arc-en-Ciel")
        if entity["label"] == "LOC" and entity["source"] == "NER":
            # Types d'√©tablissements √† d√©tecter dans le texte NER
            etab_type_patterns = {
                'ETAB_SESSAD': ['SESSAD'],
                'ETAB_EHPAD': ['EHPAD'],
                'ETAB_IME': ['IME'],
                'ETAB_MECS': ['MECS'],
                'ETAB_CHRS': ['CHRS'],
                'ETAB_CMPP': ['CMPP'],
                'ETAB_SAVS': ['SAVS'],
                'ETAB_SAMSAH': ['SAMSAH'],
                'ETAB_MAS': ['MAS'],
                'ETAB_FAM': ['FAM'],
                'ETAB_ITEP': ['ITEP'],
                'ETAB_ESAT': ['ESAT'],
            }
            
            # V√©rifier si le texte commence par un type d'√©tablissement
            text_upper = text.upper()
            for etab_category, keywords in etab_type_patterns.items():
                if any(text_upper.startswith(kw) for kw in keywords):
                    logging.debug(f"üè¢ NER TYPE+NOM: '{text}' reclass√© comme {etab_category}")
                    entity["label"] = etab_category
                    entity["category"] = etab_category
                    entity["source"] = "NER_ENHANCED"
                    return entity
        
        # Expansion d'√©tablissement UNIQUEMENT si l'ancre est pr√©sente dans le contexte proche
        # ET que le TYPE d'√©tablissement n'est PAS d√©j√† adjacent (fusion g√©r√©e s√©par√©ment)
        if entity["label"] == "LOC" and entity["source"] == "NER":
            # has_adjacent_type d√©j√† calcul√© ci-dessus
            
            if not has_adjacent_type:
                for etab_type, keywords in etab_patterns.items():
                    # V√©rifier si un mot-cl√© d'√©tablissement est dans le contexte ET proche (¬±30 caract√®res)
                    close_context = full_text[max(0, start - 30):min(len(full_text), end + 30)].lower()
                    if any(keyword in close_context for keyword in keywords):
                        # Heuristiques d'√©tablissement am√©lior√©es
                        if _is_establishment_name(text, close_context):
                            logging.debug(f"üè¢ LOC‚ÜíETAB: '{text}' reclass√© comme {etab_type}")
                            entity["label"] = etab_type
                            entity["category"] = etab_type
                            entity["source"] = "ENHANCED"
                            break
            
            # Note: Heuristique ETAB_GENERIC supprim√©e - chaque type d'√©tablissement
            # doit avoir son pattern sp√©cifique dans rules.yaml

        # Reclassifier les personnes mal √©tiquet√©es comme ORG
        if entity["label"] == "ORG" and entity["source"] == "NER":
            # Patterns de noms de personnes
            name_patterns = [
                r"\b[A-Z][a-z]+ [A-Z][A-Z]+\b",  # Pr√©nom NOM
                r"\bM\. [A-Z][A-Z]+\b",          # M. NOM
                r"\bMme [A-Z][A-Z]+\b",          # Mme NOM
                r"\bDr\. [A-Z][A-Z]+\b"         # Dr. NOM
            ]
            
            for pattern in name_patterns:
                if re.search(pattern, text):
                    logging.debug(f"üë§ ORG‚ÜíPER: '{text}' reclass√© comme personne")
                    entity["label"] = "PER"
                    entity["source"] = "ENHANCED"
                    break

        # Note: Disambiguation PER vs ETAB supprim√©e - les noms de personnes
        # d√©tect√©s par NER restent comme PER (plus de reclassification en ETAB_GENERIC)
        
        # Extension CHU : si une entit√© contient "CHU" et un nom de ville, √©tendre pour capturer le nom complet
        if entity["label"] in ["ORG", "LOC"] and entity["source"] == "NER":
            # Chercher si l'entit√© actuelle contient "CHU"
            if "chu" in text.lower():
                # Examiner le contexte apr√®s l'entit√© pour une √©ventuelle ville
                context_after = full_text[end:min(len(full_text), end + 50)]
                
                # Chercher une ville imm√©diatement apr√®s (avec espaces possibles)
                import re
                city_match = re.search(r'^\s+([A-Z][a-z]+(?:-[A-Z][a-z]+)*)', context_after)
                if city_match:
                    city_name = city_match.group(1)
                    # V√©rifier que ce n'est pas un mot courant qui suivrait CHU
                    excluded_words = ["de", "du", "des", "le", "la", "les", "et", "ou", "avec", "pour"]
                    if city_name.lower() not in excluded_words:
                        # √âtendre l'entit√© pour inclure la ville
                        extended_text = text + " " + city_name
                        entity["text"] = extended_text
                        entity["end"] = end + len(city_match.group(0))
                        entity["label"] = "ORG_CHU_CH"
                        entity["category"] = "ORG_CHU_CH"
                        entity["source"] = "ENHANCED"
                        logging.debug(f"üè• CHU extended: '{text}' ‚Üí '{extended_text}' (ORG_CHU_CH)")
                        return entity
        
        return entity

    def _get_default_priority(self, entity: Dict) -> float:
        """Retourne une priorit√© par d√©faut pour les cat√©gories non d√©finies"""
        source = entity["source"]
        label = entity["label"]
        
        if source == "ENHANCED":
            return 4.5  # Priorit√© √©lev√©e pour les am√©liorations
        elif source == "GAZETTEER":
            return 5.0
        elif source == "RULES":
            if label in ["EMAIL", "NIR", "PHONE", "DATE", "TIME"]:
                return 4.0
            elif label == "ETAB":
                return 3.5  # Priorit√© moyenne pour ETAB non sp√©cifique
            elif label == "ORG":
                return 3.0  # Priorit√© pour ORG non sp√©cifique
            else:
                return 3.0
        else:  # NER
            if label == "PER":
                return 2.0
            else:
                return 1.0

    def _apply_enhanced_filtering(self, text, entities):
        """Application du filtrage avanc√© avec les nouvelles classes"""
        if not ENHANCED_MODULES_AVAILABLE:
            return entities
        
        # 1. Filtrage par stopwords
        filtered_entities = []
        for entity in entities:
            entity_text = text[entity['start']:entity['end']]
            if not self.stopwords_filter.is_loc_stopword(entity_text):
                filtered_entities.append(entity)
            else:
                logging.debug(f"üö´ Entit√© filtr√©e (stopword): '{entity_text}'")
        
        # 2. Mise √† jour des priorit√©s si la matrice est disponible
        for entity in filtered_entities:
            text_content = text[entity['start']:entity['end']]
            entity_type = entity.get('source', '')
            enhanced_priority = self.priority_matrix.get_priority(entity_type)
            if enhanced_priority is not None:
                entity['priority'] = enhanced_priority
                logging.debug(f"üîÑ Priorit√© mise √† jour: '{text_content}' ‚Üí {enhanced_priority}")
        
        return filtered_entities

    def _contextual_disambiguation(self, full_text, entities):
        """D√©sambigu√Øsation contextuelle pour corriger les classifications √©videntes"""
        if not entities:
            return entities
        
        corrected_entities = []
        corrections_applied = 0
        
        for entity in entities:
            original_entity = dict(entity)  # Copie pour √©viter les modifications
            entity_text = entity.get('text', '')
            original_label = entity.get('label', '')
            source = entity.get('source', '')
            
            # Debug: Log toutes les entit√©s ETAB_GENERIC trouv√©es
            if (original_label.startswith('ETAB_GENERIC') and 
                (source.startswith('NER') or source == 'ENHANCED')):  # Inclure les entit√©s reclass√©es par enhance
                logging.debug(f"üîç Entit√© ETAB_GENERIC d√©tect√©e: '{entity_text}' source={source} label={original_label}")
                
                # V√©rifier si c'est un pattern de personne
                if self._is_person_pattern(entity_text):
                    logging.debug(f"   ‚úì Pattern personne d√©tect√© pour '{entity_text}'")
                    
                    # Analyse du contexte autour de l'entit√© (¬±50 caract√®res)
                    context_start = max(0, entity['start'] - 50)
                    context_end = min(len(full_text), entity['end'] + 50)
                    context = full_text[context_start:context_end].lower()
                    
                    # V√©rifier le contexte
                    if self._has_person_context(context, entity_text):
                        logging.debug(f"   ‚úì Contexte personne confirm√© pour '{entity_text}'")
                        original_entity['label'] = 'PER'
                        original_entity['source'] = 'CONTEXTUAL_PER'
                        original_entity['category'] = 'PER'
                        corrections_applied += 1
                        logging.info(f"üîÑ CORRECTION: '{entity_text}' {original_label} ‚Üí PER (contexte)")
                    else:
                        logging.debug(f"   ‚úó Pas de contexte personne pour '{entity_text}'")
                        logging.debug(f"   Contexte analys√©: '{context[:100]}...'")
                else:
                    logging.debug(f"   ‚úó Pas un pattern personne: '{entity_text}'")
            
            # Correction PER ‚Üí PROFESSION pour les titres
            elif (original_label == 'PER' or source.startswith('NER_PER')):
                context_start = max(0, entity['start'] - 50)
                context_end = min(len(full_text), entity['end'] + 50)
                context = full_text[context_start:context_end].lower()
                
                if self._has_professional_title(context, entity_text):
                    logging.debug(f"üîÑ Correction PER‚ÜíPROFESSION: '{entity_text}' (titre d√©tect√©)")
                    original_entity['label'] = 'PROFESSION'
                    original_entity['source'] = 'CONTEXTUAL_PROFESSION'
                    original_entity['category'] = 'PROFESSION'
                    corrections_applied += 1
            
            corrected_entities.append(original_entity)
        
        if corrections_applied > 0:
            logging.info(f"üéØ D√©sambigu√Øsation contextuelle: {corrections_applied} corrections appliqu√©es")
        
        return corrected_entities
    
    def _is_person_pattern(self, text):
        """D√©tecte si le texte suit un pattern de nom de personne"""
        import re
        
        # Nettoyer le texte (supprimer espaces et caract√®res parasites)
        clean_text = text.strip()
        
        # Patterns de noms de personnes avec support des accents
        patterns = [
            r'^[A-Z√Ä-√ø][a-z√†-√ø]+\s+[A-Z√Ä-√ø]{2,}$',             # Pr√©nom NOM (toutes majuscules)
            r'^[A-Z√Ä-√ø][a-z√†-√ø]+\s+[A-Z√Ä-√ø][a-z√†-√ø]+$',       # Pr√©nom Nom (style standard)
            r'^[A-Z√Ä-√ø]\.\s*[A-Z√Ä-√ø][a-z√†-√ø]+$',               # P. Nom
            r'^[A-Z√Ä-√ø][a-z√†-√ø]+\s+[A-Z√Ä-√ø][a-z√†-√ø]+(?:\s+[A-Z√Ä-√ø][a-z√†-√ø]+)?$',  # Pr√©nom Nom MiddleName
        ]
        
        for pattern in patterns:
            if re.match(pattern, clean_text):
                return True
                
        return False
    
    def _has_person_context(self, context, entity_text):
        """V√©rifie si le contexte indique qu'il s'agit d'une personne"""
        entity_lower = entity_text.strip().lower()
        
        # Civilit√©s directes
        person_indicators = [
            f'mme {entity_lower}', f'm. {entity_lower}',
            f'monsieur {entity_lower}', f'madame {entity_lower}',
            f'dr {entity_lower}', f'professeur {entity_lower}',
        ]
        
        # Fonctions apr√®s le nom
        function_indicators = [
            f'{entity_lower}, directeur', f'{entity_lower}, directrice',
            f'{entity_lower}, responsable', f'{entity_lower}, chef',
            f'{entity_lower}, tutrice', f'{entity_lower}, tuteur',
        ]
        
        # √Çge et caract√©ristiques personnelles (patterns plus flexibles)
        age_indicators = [
            ', ans', ' ans,', f'{entity_lower}, 2', f'{entity_lower}, 3',
            f'{entity_lower}, 4', f'{entity_lower}, 5', f'{entity_lower}, 6'
        ]
        
        # V√©rifier tous les indicateurs
        all_indicators = person_indicators + function_indicators + age_indicators
        
        for indicator in all_indicators:
            if indicator in context:
                logging.debug(f"üîç Indicateur personne trouv√©: '{indicator}' pour '{entity_text}'")
                return True
        
        # Debug: afficher le contexte si aucun indicateur trouv√©
        logging.debug(f"üîç Pas d'indicateur personne pour '{entity_text}' dans: '{context[:100]}...'")
        return False
    
    def _has_professional_title(self, context, entity_text):
        """V√©rifie si le contexte indique une profession"""
        # Titres professionnels avant le nom
        titles = ['dr ', 'docteur ', 'professeur ', 'pr ']
        
        for title in titles:
            if f'{title}{entity_text.lower()}' in context:
                return True
        
        return False
    
    def _merge_type_location(self, entities: List[Dict], full_text: str) -> List[Dict]:
        """
        Fusionne les TYPES d'√©tablissements (EHPAD, MAS, IME, etc.) avec les LOC/PER adjacents
        d√©tect√©s par le NER pour cr√©er des entit√©s ETAB compl√®tes.
        
        Logique:
        - Rules d√©tecte "EHPAD" seul
        - NER d√©tecte "Sainte-Gertrude" en LOC
        - Si adjacents (‚â§ 2 chars d'√©cart) ‚Üí fusionner en ETAB_EHPAD "EHPAD Sainte-Gertrude"
        - Si s√©par√©s ‚Üí ignorer le TYPE seul (filtr√© par exclusions)
        """
        logging.info(f"üîç _merge_type_location appel√©e avec {len(entities)} entit√©s")
        
        # Log des entit√©s entrantes pour debug
        for i, e in enumerate(entities[:10]):  # Limiter √† 10 pour √©viter spam
            logging.debug(f"  [{i}] {e.get('source', 'UNK'):15s} {e.get('label', 'UNK'):8s} {e.get('category', 'N/A'):20s} '{e.get('text', 'N/A')}' (start={e.get('start', 'N/A')}, end={e.get('end', 'N/A')})")
        
        # Types d'√©tablissements √† fusionner avec noms propres
        ETAB_TYPES = {
            'EHPAD', 'MAS', 'FAM', 'MECS', 'IME', 'ITEP', 'SESSAD',
            'ESAT', 'CMPP', 'CAMSP', 'FJT', 'CHRS', 'IMPRO', 'IEM', 
            'IES', 'SAFEP', 'SSEFS', 'EEAP',
            'SERVICE_SAVS', 'SERVICE_SAMSAH', 'SERVICE_SSIAD'  # Services avec noms propres (ex: SAVS "Les Passerelles")
        }
        
        # Services NON pseudonymisables (g√©n√©riques sans nom propre)
        SERVICE_TYPES = {
            'SERVICE_SPASAD', 'SERVICE_SAAD', 'SERVICE_CMP',
            'SERVICE_CATTP', 'SERVICE_CSAPA', 'SERVICE_CAARUD', 'SERVICE_PASS'
        }
        
        merged = []
        skip_indices = set()
        
        # Trier par position pour traitement s√©quentiel
        sorted_entities = sorted(enumerate(entities), key=lambda x: x[1]['start'])
        
        for i, (idx, entity) in enumerate(sorted_entities):
            if idx in skip_indices:
                continue
            
            # V√©rifier si c'est un TYPE d'√©tablissement d√©tect√© par RULES
            is_etab_type = (
                entity['source'] == 'RULES' and 
                entity.get('category') in ETAB_TYPES
            )
            
            if is_etab_type and i + 1 < len(sorted_entities):
                # R√©cup√©rer l'entit√© suivante
                next_idx, next_entity = sorted_entities[i + 1]
                
                # Calculer l'√©cart entre les deux entit√©s
                gap = next_entity['start'] - entity['end']
                
                # V√©rifier si l'entit√© suivante est un LOC ou PER du NER et est adjacente
                is_adjacent_name = (
                    next_entity['label'] in {'LOC', 'PER'} and
                    next_entity['source'] == 'NER' and
                    gap <= 6  # Max 6 chars: guillemets normalis√©s " ", espaces, apostrophes
                )
                
                # Log pour debug
                if entity.get('category') == 'SESSAD':
                    logging.info(f"üîç SESSAD: '{entity['text']}' end={entity['end']}, next='{next_entity.get('text', 'N/A')}' start={next_entity.get('start', 'N/A')}, gap={gap}, is_adjacent={is_adjacent_name}")
                
                if is_adjacent_name:
                    # FUSION !
                    type_category = entity.get('category')
                    type_text = entity['text']
                    name_text = next_entity['text']
                    
                    # Construire le texte complet de l'entit√© fusionn√©e
                    merged_start = entity['start']
                    merged_end = next_entity['end']
                    merged_text = full_text[merged_start:merged_end]
                    
                    # Log de fusion pour debug
                    logging.info(f"üîó Fusion: '{type_text}' (start={merged_start}, end={entity['end']}) + '{name_text}' (start={next_entity['start']}, end={merged_end}) ‚Üí '{merged_text}'")
                    
                    # Cat√©gorie finale : ETAB_<TYPE> (ne pas doubler si d√©j√† ETAB_)
                    final_category = type_category if type_category.startswith('ETAB_') else f"ETAB_{type_category}"
                    
                    merged_entity = {
                        'text': merged_text,
                        'start': merged_start,
                        'end': merged_end,
                        'label': 'ETAB',
                        'category': final_category,
                        'source': 'MERGED_TYPE_LOC',
                        'score': max(entity.get('score', 1.0), next_entity.get('score', 0.8))
                    }
                    
                    merged.append(merged_entity)
                    skip_indices.add(next_idx)  # Ignorer l'entit√© suivante (d√©j√† fusionn√©e)
                    
                    logging.debug(f"üîó Fusion TYPE+LOC: '{type_text}' + '{name_text}' ‚Üí '{merged_text}' ({final_category})")
                    continue
                    
            # Pas de fusion possible : garder l'entit√© originale SAUF si c'est un TYPE seul
            if entity['source'] == 'RULES' and entity.get('category') in ETAB_TYPES:
                # TYPE seul sans nom adjacent ‚Üí ne pas pseudonymiser (filtrage implicite)
                logging.debug(f"üö´ TYPE seul ignor√© (pas de nom adjacent): '{entity['text']}' ({entity.get('category')})")
                continue
            
            # Services g√©n√©riques : conserver mais NE PAS pseudonymiser
            if entity.get('category') in SERVICE_TYPES:
                logging.debug(f"‚ÑπÔ∏è Service g√©n√©rique d√©tect√© (non pseudonymis√©): '{entity['text']}' ({entity.get('category')})")
                # On ne l'ajoute PAS √† merged pour √©viter la pseudonymisation
                continue
            
            # Entit√© valide : ajouter
            merged.append(entity)
        
        logging.info(f"üîó Fusion TYPE+LOC: {len(entities)} ‚Üí {len(merged)} entit√©s ({len(entities) - len(merged)} filtr√©es)")
        return merged
    
    def resolve_conflicts(self, ner_entities: List[Dict], rules_entities: List[Dict], gazetteer_entities: List[Dict] = None, full_text: str = "") -> List[Dict]:
        """R√©sout les conflits entre NER, r√®gles et gazetteers avec filtrage avanc√© et d√©sambigu√Øsation"""
        all_entities = ner_entities + rules_entities
        if gazetteer_entities:
            all_entities.extend(gazetteer_entities)
        
        # √âtape 1 : Appliquer le filtrage avanc√© si disponible  
        if ENHANCED_MODULES_AVAILABLE and full_text:
            all_entities = self._apply_enhanced_filtering(full_text, all_entities)
        
        # √âtape 2 : Filtrer les entit√©s invalides et am√©liorer la classification
        valid_entities = []
        for entity in all_entities:
            if self._is_valid_entity(entity):
                # Am√©liorer la classification
                enhanced_entity = self._enhance_entity_classification(entity, full_text)
                valid_entities.append(enhanced_entity)
        
        # √âtape 3 : D√©sambigu√Øsation contextuelle (APR√àS l'am√©lioration pour corriger les erreurs)
        if full_text:
            valid_entities = self._contextual_disambiguation(full_text, valid_entities)
        
        # √âtape 4 : Fusion TYPE + LOC/PER pour √©tablissements (NOUVELLE LOGIQUE)
        if full_text:
            valid_entities = self._merge_type_location(valid_entities, full_text)
        
        resolved = []
        
        # Trier par position
        valid_entities.sort(key=lambda x: x["start"])
        
        for current in valid_entities:
            # V√©rifier les conflits avec les entit√©s d√©j√† r√©solues
            conflicts = []
            for resolved_entity in resolved:
                overlap = self._calculate_overlap(current, resolved_entity)
                if overlap > 0.3:  # Seuil de conflit
                    conflicts.append(resolved_entity)
            
            if not conflicts:
                # Pas de conflit, ajouter directement
                resolved.append(current)
            else:
                # R√©soudre le conflit : LONGEST-SPAN-WINS d'abord, puis priorit√©
                current_length = current['end'] - current['start']
                current_priority = self._get_priority(current)
                
                should_replace = True
                for conflict in conflicts:
                    conflict_length = conflict['end'] - conflict['start']
                    conflict_priority = self._get_priority(conflict)
                    
                    # R√®gle 1 : Longest-span-wins (si diff√©rence > 3 chars)
                    if abs(current_length - conflict_length) > 3:
                        if conflict_length > current_length:
                            should_replace = False
                            break
                    # R√®gle 2 : Si longueurs similaires, utiliser la priorit√©
                    elif conflict_priority >= current_priority:
                        should_replace = False
                        break
                
                if should_replace:
                    # Retirer les entit√©s conflictuelles moins prioritaires/courtes
                    for conflict in conflicts:
                        resolved.remove(conflict)
                    resolved.append(current)
                    
                    logging.debug(f"üîÑ Conflit r√©solu: '{current['text']}' ({self._get_priority_key(current)}, len={current_length}) remplace {len(conflicts)} entit√©(s)")
        
        # Statistiques
        stats = {}
        for entity in resolved:
            key = self._get_priority_key(entity)
            stats[key] = stats.get(key, 0) + 1
        
        logging.info(f"‚öñÔ∏è Conflits r√©solus: {len(resolved)} entit√©s finales {stats}")
        
        return resolved
    


def pseudonymize_text(text: str, entities: List[Dict], store: PseudonymStore) -> str:
    """Applique la pseudonymisation sur le texte"""
    entities_sorted = sorted(entities, key=lambda x: x["start"], reverse=True)
    
    result = text
    replacements = 0
    
    for entity in entities_sorted:
        start, end = entity["start"], entity["end"]
        original = entity["text"]
        label = entity["label"]
        category = entity.get("category")  # R√©cup√©rer la cat√©gorie sp√©cifique
        
        actual_text = result[start:end]
        if actual_text == original:
            replacement = store.pseudonymize(original, label, category)
            result = result[:start] + replacement + result[end:]
            replacements += 1
            source_info = f"({entity.get('source', 'UNK')})"
            logging.debug(f"üîÑ {original} ‚Üí {replacement} {source_info}")
        else:
            logging.warning(f"‚ö†Ô∏è Mismatch pos {start}-{end}: '{original}' vs '{actual_text}'")
    
    logging.info(f"‚úÖ {replacements} remplacements effectu√©s")
    return result


def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(description="Pipeline complet NER + R√®gles + Gazetteers")
    parser.add_argument("--input", required=True, help="Fichier d'entr√©e")
    parser.add_argument("--output", required=True, help="Fichier de sortie")
    parser.add_argument("--mapping", help="Fichier de mapping")
    parser.add_argument("--load-mapping", help="Fichier de mapping existant √† charger")
    parser.add_argument("--rules", default="rules/rules.yaml", help="Fichier de r√®gles")
    parser.add_argument("--gazetteers", default="gazetteer", help="Dossier des gazetteers")
    parser.add_argument("--model", default="Jean-Baptiste/camembert-ner", help="Mod√®le NER")
    parser.add_argument("--log-level", choices=["DEBUG", "INFO", "WARNING"], default="INFO")
    parser.add_argument("--depseudonymize", action="store_true", help="Mode d√©pseudonymisation")
    
    args = parser.parse_args()
    
    # Configuration logging
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%H:%M:%S"
    )
    
    if args.depseudonymize:
        logging.info("üîì Mode d√©pseudonymisation")
        if not args.load_mapping:
            logging.error("‚ùå Mode d√©pseudonymisation n√©cessite --load-mapping")
            sys.exit(1)
    else:
        logging.info("üöÄ Pipeline complet NER + R√®gles + Gazetteers")
    
    # Charger le texte
    text = Path(args.input).read_text(encoding="utf-8")
    
    # Supprimer les guillemets typographiques pour am√©liorer la d√©tection NER
    # (les guillemets confondent le NER qui inclut le type d'√©tablissement dans la d√©tection)
    original_text = text
    text = text.replace('¬´', '').replace('¬ª', '').replace('"', '').replace('"', '').replace('"', '')
    
    if text != original_text:
        logging.info(f"üìù Guillemets supprim√©s pour am√©liorer d√©tection NER")
    logging.info(f"üìñ Texte charg√©: {len(text)} caract√®res")
    
    # Normalisation Unicode si les modules avanc√©s sont disponibles
    original_length = len(text)
    if ENHANCED_MODULES_AVAILABLE:
        normalizer = TextNormalizer()
        text = normalizer.normalize_unicode(text)
        if len(text) != original_length:
            logging.info(f"üîÑ Normalisation Unicode: {original_length} ‚Üí {len(text)} caract√®res")
    
    # Mode d√©pseudonymisation
    if args.depseudonymize:
        store = PseudonymStore.load(args.load_mapping)
        result = store.depseudonymize(text)
        logging.info("üîì D√©pseudonymisation termin√©e")
    else:
        # Mode pseudonymisation
        
        # Charger mapping existant si demand√©
        if args.load_mapping:
            store = PseudonymStore.load(args.load_mapping)
        else:
            store = PseudonymStore()
        
        # D√©tection NER
        ner = ChunkedNER(args.model)
        ner_entities = ner.detect_entities(text)
        
        # D√©tection par r√®gles
        rules = RulesEngine(args.rules)
        rules_entities = rules.detect_entities(text)
        
        # D√©tection par gazetteers
        gazetteers = GazetteerEngine(args.gazetteers)
        gazetteer_entities = gazetteers.detect_entities(text)
        
        # Charger les exclusions depuis le gazetteer_exclusions
        exclusions = []
        if 'gazetteer_exclusions' in gazetteers.gazetteers:
            exclusions = [entry['name'] for entry in gazetteers.gazetteers['gazetteer_exclusions']]
            logging.info(f"üö´ {len(exclusions)} exclusions charg√©es")
        
        # R√©solution des conflits
        resolver = ConflictResolver(exclusions=exclusions)
        final_entities = resolver.resolve_conflicts(ner_entities, rules_entities, gazetteer_entities, text)
        
        # Pseudonymisation
        result = pseudonymize_text(text, final_entities, store)
    
    # Sauvegarde
    Path(args.output).write_text(result, encoding="utf-8")
    logging.info(f"üíæ R√©sultat sauv√©: {args.output}")
    
    if args.mapping and not args.depseudonymize:
        store.save(args.mapping)
    
    # Statistiques finales
    if not args.depseudonymize:
        logging.info(f"üìä Statistiques: {store.counters}")
    logging.info("üéâ Pipeline termin√©!")


if __name__ == "__main__":
    main()