#!/usr/bin/env python3
"""
Pipeline de pseudonymisation hybride : NER chunked + R√®gles avec r√©solution de conflits
"""
import argparse
import csv
import json
import logging
import re
import sys
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Set
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline, CamembertTokenizer

# Import des nouveaux modules d'am√©lioration
try:
    from text_processing import TextNormalizer, StopWordsFilter, PriorityMatrix
    ENHANCED_MODULES_AVAILABLE = True
except ImportError:
    # Fallback si les modules ne sont pas disponibles
    TextNormalizer = None
    StopWordsFilter = None
    PriorityMatrix = None
    ENHANCED_MODULES_AVAILABLE = False


def _norm_key(s: str) -> str:
    """Normalise une cl√© pour l'indexation en supprimant les espaces multiples et normalisant les apostrophes"""
    return re.sub(r"\s+", " ", s.replace("'", "'")).strip()


def _is_establishment_name(text: str, context: str) -> bool:
    """D√©termine si un nom ressemble √† un √©tablissement bas√© sur des heuristiques"""
    # Normaliser le texte
    norm_text = _norm_key(text)
    
    # Heuristique 1: Commence par des articles caract√©ristiques d'√©tablissements
    if norm_text.startswith(("Les ", "Le ", "La ", "L'")):
        # Exclure les √©coles g√©n√©rales et les villes connues
        if (not norm_text.lower().startswith(("√©cole", "coll√®ge", "lyc√©e")) and
            not any(city in norm_text.lower() for city in [
                "paris", "lyon", "marseille", "toulouse", "nice", "nantes", 
                "montpellier", "strasbourg", "bordeaux", "lille", "rennes", 
                "reims", "nanterre", "bobigny", "cr√©teil", "montreuil"
            ])):
            return True
    
    # Heuristique 2: Contexte contient des verbes indicateurs d'√©tablissement
    establishment_verbs = [
        "accueille", "accompagne", "h√©berge", "prend en charge", "suit", 
        "oriente", "√©value", "propose", "offre", "dispense", "assure"
    ]
    if any(verb in context for verb in establishment_verbs):
        return True
    
    # Heuristique 3: Noms d'arbres/plantes (souvent utilis√©s pour les √©tablissements)
    plant_names = [
        "tilleuls", "peupliers", "ch√™nes", "ormes", "platanes", "√©rables",
        "acacias", "cypr√®s", "pins", "sapins", "bouleaux", "fr√™nes",
        "roses", "lilas", "jasmin", "glycines", "magnolias"
    ]
    if any(plant in norm_text.lower() for plant in plant_names):
        return True
    
    return False


class PseudonymStore:
    """Gestionnaire de pseudonymisation avec compteurs et sauvegarde"""
    
    def __init__(self):
        self.forward = {}  # original -> pseudonyme
        self.reverse = {}  # pseudonyme -> original
        self.counters = {"PER": 0, "ORG": 0, "LOC": 0, "ETAB": 0, "DATE": 0, "PHONE": 0, "NIR": 0, "TIME": 0, "EMAIL": 0}
    
    def pseudonymize(self, text: str, label: str, category: str = None) -> str:
        """Pseudonymise un texte ou retourne le pseudonyme existant"""
        if text in self.forward:
            return self.forward[text]
        
        # Utiliser la cat√©gorie sp√©cifique si fournie, sinon utiliser le label g√©n√©rique
        pseudo_label = category if category else label
        
        # Cr√©er le compteur pour cette cat√©gorie si n√©cessaire
        if pseudo_label not in self.counters:
            self.counters[pseudo_label] = 0
            
        self.counters[pseudo_label] += 1
        replacement = f"<{pseudo_label}_{self.counters[pseudo_label]}>"
        self.forward[text] = replacement
        self.reverse[replacement] = text
        return replacement
    
    def save(self, path: str):
        """Sauvegarde le mapping au format JSON"""
        data = {
            "forward": self.forward,
            "reverse": self.reverse,
            "counters": self.counters,
            "stats": {
                "total_entities": len(self.forward),
                "per_type": dict(self.counters)
            }
        }
        
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logging.info(f"üíæ Mapping sauv√©: {path}")
    
    @classmethod
    def load(cls, path: str) -> "PseudonymStore":
        """Charge un mapping existant"""
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        store = cls()
        store.forward = data.get("forward", {})
        store.reverse = data.get("reverse", {})
        store.counters = data.get("counters", {"PER": 0, "ORG": 0, "LOC": 0, "ETAB": 0})
        
        logging.info(f"üìÇ Mapping charg√©: {path}")
        return store
    
    def depseudonymize(self, text: str) -> str:
        """Restaure le texte original √† partir des pseudonymes"""
        result = text
        for pseudonym, original in self.reverse.items():
            result = result.replace(pseudonym, original)
        return result


class GazetteerEngine:
    """Moteur de gazetteers pour d√©tecter les entit√©s par correspondance exacte"""
    
    def __init__(self, gazetteer_dir: str):
        self.gazetteer_dir = Path(gazetteer_dir)
        self.gazetteers = {}
        self._load_gazetteers()
    
    def _load_gazetteers(self):
        """Charge tous les fichiers CSV de gazetteers"""
        if not self.gazetteer_dir.exists():
            logging.warning(f"‚ö†Ô∏è Dossier gazetteers introuvable: {self.gazetteer_dir}")
            return
        
        csv_files = list(self.gazetteer_dir.glob("*.csv"))
        if not csv_files:
            logging.warning(f"‚ö†Ô∏è Aucun fichier CSV trouv√© dans: {self.gazetteer_dir}")
            return
        
        for csv_file in csv_files:
            gazetteer_name = csv_file.stem
            entries = []
            
            try:
                with open(csv_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        if 'name' in row and 'category' in row:
                            entries.append({
                                'name': row['name'].strip(),
                                'category': row['category'].strip()
                            })
                        elif 'keyword' in row and 'category' in row:
                            entries.append({
                                'name': row['keyword'].strip(),
                                'category': row['category'].strip()
                            })
                
                if entries:
                    self.gazetteers[gazetteer_name] = entries
                    logging.info(f"üìö Gazetteer {gazetteer_name}: {len(entries)} entr√©es")
                
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Erreur chargement {csv_file}: {e}")
        
        total_entries = sum(len(entries) for entries in self.gazetteers.values())
        logging.info(f"‚úÖ Gazetteers charg√©s: {len(self.gazetteers)} fichiers, {total_entries} entr√©es")
    
    def detect_entities(self, text: str) -> List[Dict]:
        """D√©tecte les entit√©s selon les gazetteers par correspondance exacte"""
        entities = []
        
        for gazetteer_name, entries in self.gazetteers.items():
            for entry in entries:
                name = entry['name']
                category = entry['category']
                
                # Recherche case-insensitive avec correspondance de mots entiers
                pattern = r'\b' + re.escape(name) + r'\b'
                
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    # D√©terminer le label final
                    if category.startswith('ORG_'):
                        label = 'ORG'
                    elif category.startswith('ETAB_'):
                        label = 'ETAB'
                    else:
                        label = 'ORG'  # Par d√©faut
                    
                    entities.append({
                        "start": match.start(),
                        "end": match.end(),
                        "text": match.group(),
                        "label": label,
                        "source": "GAZETTEER",
                        "category": category,
                        "gazetteer": gazetteer_name,
                        "score": 1.0  # Score maximum pour gazetteers
                    })
        
        logging.info(f"üìñ Gazetteers d√©tect√©s: {len(entities)} entit√©s")
        return entities


class RulesEngine:
    """Moteur de r√®gles pour d√©tecter les organisations, √©tablissements et donn√©es sensibles"""
    
    def __init__(self, rules_file: str):
        self.rules_file = rules_file
        self.org_patterns = {}
        self.etab_patterns = {}
        self.date_patterns = {}
        self.phone_patterns = {}
        self.nir_patterns = {}
        self.time_patterns = {}
        self.email_patterns = {}
        self.profession_patterns = {}
        self._load_rules()
    
    def _load_rules(self):
        """Charge les r√®gles depuis le fichier YAML"""
        if not Path(self.rules_file).exists():
            logging.warning(f"‚ö†Ô∏è Fichier de r√®gles introuvable: {self.rules_file}")
            return
        
        with open(self.rules_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # R√®gles ORG
        org_rules = data.get('org_regex', {})
        for category, patterns in org_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.org_patterns[category] = compiled_patterns
        
        # R√®gles ETAB
        etab_rules = data.get('etab_categories', {})
        for category, patterns in etab_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.etab_patterns[category] = compiled_patterns
        
        # R√®gles DATE
        date_rules = data.get('date_regex', {})
        for category, patterns in date_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.date_patterns[category] = compiled_patterns
        
        # R√®gles PHONE
        phone_rules = data.get('phone_regex', {})
        for category, patterns in phone_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.phone_patterns[category] = compiled_patterns
        
        # R√®gles NIR
        nir_rules = data.get('nir_regex', {})
        for category, patterns in nir_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.nir_patterns[category] = compiled_patterns
        
        # R√®gles TIME
        time_rules = data.get('time_regex', {})
        for category, patterns in time_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.time_patterns[category] = compiled_patterns
        
        # R√®gles EMAIL
        email_rules = data.get('email_regex', {})
        for category, patterns in email_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.email_patterns[category] = compiled_patterns
        
        # R√®gles PROFESSION
        profession_rules = data.get('profession_regex', {})
        for category, patterns in profession_rules.items():
            compiled_patterns = []
            for pattern in patterns:
                try:
                    compiled_patterns.append(re.compile(pattern, re.IGNORECASE))
                except re.error as e:
                    logging.warning(f"‚ö†Ô∏è Pattern invalide '{pattern}': {e}")
            
            if compiled_patterns:
                self.profession_patterns[category] = compiled_patterns
        
        total_patterns = len(self.org_patterns) + len(self.etab_patterns) + len(self.date_patterns) + len(self.phone_patterns) + len(self.nir_patterns) + len(self.time_patterns) + len(self.email_patterns) + len(self.profession_patterns)
        logging.info(f"‚úÖ R√®gles charg√©es: {len(self.org_patterns)} ORG, {len(self.etab_patterns)} ETAB, {len(self.date_patterns)} DATE, {len(self.phone_patterns)} PHONE, {len(self.nir_patterns)} NIR, {len(self.time_patterns)} TIME, {len(self.email_patterns)} EMAIL, {len(self.profession_patterns)} PROFESSION")
    
    def detect_entities(self, text: str) -> List[Dict]:
        """D√©tecte les entit√©s selon les r√®gles avec gestion des entit√©s compos√©es"""
        entities = []
        
        # Patterns de localisation pour d√©tecter les entit√©s compos√©es
        loc_patterns = [
            r"\b(√†|au|aux|dans|sur|de|du|des)\s+[A-Z√â√à√Ä√Ç√é√ô√î√á][A-Za-z√Ä-√ñ√ò-√∂√∏-√ø''\-\s]+\b",
            r"\bde\s+[A-Z√â√à√Ä√Ç√é√ô√î√á][A-Za-z√Ä-√ñ√ò-√∂√∏-√ø''\-\s]+\b"
        ]
        
        # D√©tecter les ORG avec extension g√©ographique
        for category, patterns in self.org_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    start, end = match.start(), match.end()
                    base_text = match.group()
                    
                    # Chercher une extension g√©ographique apr√®s l'entit√©
                    extended_text = base_text
                    extended_end = end
                    
                    # Chercher "de/du/des + lieu" apr√®s l'entit√© de base
                    remaining_text = text[end:]
                    for loc_pattern in loc_patterns:
                        loc_match = re.match(r'\s*' + loc_pattern, remaining_text)
                        if loc_match:
                            extension = loc_match.group().strip()
                            extended_text = base_text + " " + extension
                            extended_end = end + loc_match.end()
                            break
                    
                    entities.append({
                        "start": start,
                        "end": extended_end,
                        "text": extended_text,
                        "label": "ORG",
                        "source": "RULES",
                        "category": category,  # Cat√©gorie sp√©cifique
                        "score": 1.0
                    })
        
        # D√©tecter les ETAB avec extension g√©ographique
        for category, patterns in self.etab_patterns.items():
            for pattern in patterns:
                for match in pattern.finditer(text):
                    start, end = match.start(), match.end()
                    base_text = match.group()
                    
                    # Chercher une extension g√©ographique apr√®s l'entit√©
                    extended_text = base_text
                    extended_end = end
                    
                    # Chercher "de/du/des + lieu" apr√®s l'entit√© de base
                    remaining_text = text[end:]
                    for loc_pattern in loc_patterns:
                        loc_match = re.match(r'\s*' + loc_pattern, remaining_text)
                        if loc_match:
                            extension = loc_match.group().strip()
                            extended_text = base_text + " " + extension
                            extended_end = end + loc_match.end()
                            break
                    
                    entities.append({
                        "start": start,
                        "end": extended_end,
                        "text": extended_text,
                        "label": "ETAB",
                        "source": "RULES",
                        "category": category,  # Cat√©gorie sp√©cifique
                        "score": 1.0
                    })
        
        # D√©tecter les autres types de donn√©es sensibles (sans extension g√©ographique)
        sensitive_data_types = [
            ("date_patterns", "DATE"),
            ("phone_patterns", "PHONE"),
            ("nir_patterns", "NIR"),
            ("time_patterns", "TIME"),
            ("email_patterns", "EMAIL"),
            ("profession_patterns", "PROFESSION")
        ]
        
        for pattern_attr, label in sensitive_data_types:
            patterns_dict = getattr(self, pattern_attr, {})
            for category, patterns in patterns_dict.items():
                for pattern in patterns:
                    for match in pattern.finditer(text):
                        entities.append({
                            "start": match.start(),
                            "end": match.end(),
                            "text": match.group(),
                            "label": label,
                            "source": "RULES",
                            "category": category,
                            "score": 1.0
                        })
        
        logging.info(f"üîß R√®gles d√©tect√©es: {len(entities)} entit√©s")
        return entities


class ChunkedNER:
    """D√©tecteur NER avec d√©coupage en chunks"""
    
    def __init__(self, model_name: str, max_tokens: int = 400):
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.tokenizer = None
        self.pipeline = None
        self._load_model()
    
    def _load_model(self):
        """Charge le mod√®le NER"""
        logging.info(f"ü§ñ Chargement mod√®le NER: {self.model_name}")
        
        try:
            # Essayer d'abord avec AutoTokenizer fast
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Tokenizer fast √©chou√©, utilisation CamembertTokenizer: {e}")
            # Fallback vers CamembertTokenizer explicite
            self.tokenizer = CamembertTokenizer.from_pretrained(self.model_name)
        
        model = AutoModelForTokenClassification.from_pretrained(self.model_name)
        self.pipeline = pipeline(
            "token-classification", 
            model=model, 
            tokenizer=self.tokenizer, 
            aggregation_strategy="simple"
        )
        
        logging.info("‚úÖ Mod√®le NER charg√©")
    
    def _split_into_chunks(self, text: str) -> List[Tuple[str, int, int]]:
        """D√©coupe le texte en chunks avec positions"""
        sentences = text.split('. ')
        chunks = []
        current_pos = 0
        
        i = 0
        while i < len(sentences):
            chunk_sentences = []
            chunk_tokens = 0
            
            while i < len(sentences) and chunk_tokens < self.max_tokens:
                sentence = sentences[i]
                sentence_tokens = len(self.tokenizer.encode(sentence, add_special_tokens=False))
                
                if chunk_tokens + sentence_tokens > self.max_tokens + 50 and chunk_sentences:
                    break
                
                chunk_sentences.append(sentence)
                chunk_tokens += sentence_tokens
                i += 1
            
            if not chunk_sentences:
                chunk_sentences = [sentences[i]]
                i += 1
            
            chunk_text = '. '.join(chunk_sentences)
            if not chunk_text.endswith('.'):
                chunk_text += '.'
            
            chunk_start = current_pos
            chunk_end = chunk_start + len(chunk_text)
            
            chunks.append((chunk_text, chunk_start, chunk_end))
            
            current_pos += len(chunk_text)
            if i < len(sentences):
                current_pos += 1
        
        logging.info(f"üìè Texte divis√© en {len(chunks)} chunks")
        return chunks

    def detect_entities(self, text: str) -> List[Dict]:
        """D√©tecte les entit√©s NER avec d√©coupage en chunks"""
        chunks = self._split_into_chunks(text)
        all_entities = []
        
        for chunk_idx, (chunk_text, chunk_start, chunk_end) in enumerate(chunks):
            logging.debug(f"üß© Traitement chunk {chunk_idx + 1}/{len(chunks)}")
            
            try:
                chunk_entities = self.pipeline(chunk_text)
                logging.debug(f"üîç Chunk {chunk_idx} NER r√©sultat brut: {chunk_entities}")
                
                for entity in chunk_entities:
                    label = entity.get("entity_group")
                    if label in {"PER", "ORG", "LOC"}:
                        if entity.get("start") is None or entity.get("end") is None:
                            logging.warning(f"‚ö†Ô∏è Entit√© avec start/end None: {entity}")
                            continue
                            
                        global_start = chunk_start + int(entity["start"])
                        global_end = chunk_start + int(entity["end"])
                        entity_text = text[global_start:global_end]
                        
                        all_entities.append({
                            "start": global_start,
                            "end": global_end,
                            "text": entity_text,
                            "label": label,
                            "source": "NER",
                            "score": float(entity.get("score", 0.0)),
                            "chunk": chunk_idx
                        })
                        
            except Exception as ex:
                logging.warning(f"‚ö†Ô∏è Erreur chunk {chunk_idx}: {ex}")
        
        logging.info(f"ü§ñ NER d√©tect√©: {len(all_entities)} entit√©s")
        return all_entities


class ConflictResolver:
    """R√©solveur de conflits entre NER, r√®gles et gazetteers avec syst√®me de priorit√©"""
    
    def __init__(self):
        # Priorit√©s : plus haut = prioritaire (matrice am√©lior√©e)
        self.priorities = {
            # Donn√©es sensibles critiques
            "RULES_EMAIL": 10.0,
            "RULES_PHONE": 9.5,
            "RULES_NIR": 9.4,
            "ENHANCED_ADDR_FULL": 9.3,
            "RULES_ADDR_FULL": 9.2,
            
            # Gazetteers - priorit√© absolue pour entit√©s connues
            "GAZETTEER_ORG": 9.0,
            "GAZETTEER_ETAB": 9.0,
            
            # Organisations sp√©cialis√©es (priorit√© √©lev√©e)
            "RULES_ORG_CHU_CH": 8.5,
            "ENHANCED_ORG_CHU_CH": 8.5,
            "RULES_ORG_JUSTICE": 8.4,
            "RULES_ORG_MDPH": 8.3,
            "RULES_ORG_ARS": 8.2,
            "RULES_ORG_DEPARTEMENT": 8.1,
            "RULES_ORG_PREFECTURE": 8.0,
            
            # Sources contextuelles (priorit√© √©lev√©e car tr√®s fiables)
            "CONTEXTUAL_PER": 7.9,
            "CONTEXTUAL_PROFESSION": 7.8,
            
            # √âtablissements sp√©cialis√©s
            "RULES_EHPAD": 7.9,
            "RULES_IME": 7.8,
            "RULES_ITEP": 7.7,
            "RULES_MECS": 7.6,
            "RULES_ESAT": 7.5,
            "RULES_PROFESSION": 7.0,  # Priorit√© √©lev√©e pour √©viter ORG
            
            # √âtablissements avec noms (ENHANCED)
            "ENHANCED_ETAB_MECS": 6.8,
            "ENHANCED_ETAB_IME": 6.7,
            "ENHANCED_ETAB_ITEP": 6.6,
            "ENHANCED_ETAB_SESSAD": 6.5,
            "ENHANCED_ETAB_ESAT": 6.4,
            "ENHANCED_ETAB_EHPAD": 6.3,
            "ENHANCED_ETAB_MAS": 6.2,
            "ENHANCED_ETAB_FAM": 6.1,
            "ENHANCED_ETAB_FOYER_VIE": 6.0,
            
            # Adresses sp√©cialis√©es
            "ENHANCED_ADDR_STREET": 5.5,
            "RULES_ADDR_STREET": 5.4,
            "ENHANCED_LOC_CITY": 5.0,
            
            # Temporel
            "RULES_DATE": 4.5,
            "RULES_TIME": 4.4,
            
            # Localisation (priorit√© plus basse)
            "NER_LOC_CITY": 3.5,
            "NER_LOC": 3.0,
            
            # NER et r√®gles g√©n√©riques
            "NER_PER": 2.5,
            "RULES_ORG": 2.0,
            "RULES_ETAB": 2.0,
            "NER_ORG": 1.5,
            "DEFAULT": 1.0
        }
        
        # Initialiser les modules d'am√©lioration si disponibles
        if ENHANCED_MODULES_AVAILABLE:
            self.text_normalizer = TextNormalizer()
            self.stopwords_filter = StopWordsFilter()
            self.priority_matrix = PriorityMatrix()
        else:
            self.text_normalizer = None
            self.stopwords_filter = None
            self.priority_matrix = None
    
    def _calculate_overlap(self, entity1: Dict, entity2: Dict) -> float:
        """Calcule le taux de chevauchement entre deux entit√©s"""
        start1, end1 = entity1["start"], entity1["end"]
        start2, end2 = entity2["start"], entity2["end"]
        
        overlap_start = max(start1, start2)
        overlap_end = min(end1, end2)
        
        if overlap_start >= overlap_end:
            return 0.0
        
        overlap_length = overlap_end - overlap_start
        min_length = min(end1 - start1, end2 - start2)
        
        return overlap_length / min_length if min_length > 0 else 0.0
    
    def _get_priority_key(self, entity: Dict) -> str:
        """G√©n√®re la cl√© de priorit√© pour une entit√©"""
        source = entity["source"]
        label = entity["label"]
        category = entity.get("category")
        
        if source == "GAZETTEER":
            return f"GAZETTEER_{label}"
        elif source == "RULES":
            # Utiliser la cat√©gorie sp√©cifique si disponible
            if category:
                return f"RULES_{category}"
            else:
                return f"RULES_{label}"
        else:
            return f"NER_{label}"
    
    def _get_priority(self, entity: Dict) -> float:
        """Retourne la priorit√© d'une entit√©"""
        priority_key = self._get_priority_key(entity)
        return self.priorities.get(priority_key, self._get_default_priority(entity))
    
    def _is_valid_entity(self, entity: Dict) -> bool:
        """Filtre les entit√©s invalides"""
        text = entity["text"].strip()
        
        # √âliminer les cha√Ænes vides ou trop courtes
        if len(text) < 2:
            logging.debug(f"üóëÔ∏è Entit√© trop courte ignor√©e: '{text}'")
            return False
            
        # √âliminer les entit√©s qui sont juste des lettres isol√©es pour ORG
        if entity["label"] == "ORG" and len(text) <= 2 and text.isalpha():
            logging.debug(f"üóëÔ∏è ORG trop court ignor√©: '{text}'")
            return False
        
        # √âliminer les abr√©viations ambigu√´s sans contexte clair
        ambiguous_short = ["ME", "C", "M", "AS", "ES", "IS"]
        if text.strip() in ambiguous_short and entity["label"] in ["ORG", "ETAB"]:
            logging.debug(f"üóëÔ∏è Abr√©viation ambigu√´ ignor√©e: '{text}'")
            return False
            
        # √âliminer les intitul√©s de fonction/m√©tier 
        job_titles = [
            "technicien intervention sociale et familiale",
            "assistant familial", "√©ducateur", "psychologue",
            "travailleur social", "aide soignant", "conseiller",
            "r√©f√©rent", "coordinateur", "superviseur"
        ]
        
        if any(job in text.lower() for job in job_titles):
            logging.debug(f"üóëÔ∏è Intitul√© de fonction ignor√©: '{text}'")
            return False
        
        # √âliminer les doublons d'organisations (garder la version la plus sp√©cifique)
        if entity["label"] == "ORG" and entity.get("category") != "ORG_ENTREPRISE_PRIV":
            # Si c'est un nom g√©n√©rique d'organisation d√©j√† couvert par une r√®gle sp√©cialis√©e
            generic_orgs = ["conseil d√©partemental", "d√©partement", "prefecture"]
            if text.lower() in generic_orgs:
                logging.debug(f"üóëÔ∏è Organisation g√©n√©rique ignor√©e (r√®gle sp√©cialis√©e existe): '{text}'")
                return False
            
        return True
    
    def _enhance_entity_classification(self, entity: Dict, full_text: str) -> Dict:
        """Am√©liore la classification des entit√©s selon le contexte"""
        text = entity["text"]
        start = entity["start"]
        end = entity["end"]
        
        # R√©cup√©rer le contexte √©largi (¬±50 caract√®res pour √™tre plus pr√©cis)
        context_start = max(0, start - 50)
        context_end = min(len(full_text), end + 50)
        context = full_text[context_start:context_end].lower()
        
        # Patterns d'√©tablissements pour expansion PR√âCISE
        etab_patterns = {
            "ETAB_MECS": ["mecs", "maison d'enfants"],
            "ETAB_IME": ["ime"],
            "ETAB_ITEP": ["itep"],
            "ETAB_SESSAD": ["sessad"],
            "ETAB_ESAT": ["esat", "√©tablissement et service d'aide par le travail"],
            "ETAB_EHPAD": ["ehpad", "r√©sidence m√©dicalis√©e"],
            "ETAB_FOYER_VIE": ["foyer de vie"],
            "ETAB_MAS": ["mas", "maison d'accueil sp√©cialis√©e"],
            "ETAB_FAM": ["fam", "foyer d'accueil m√©dicalis√©"]
        }
        
        # Patterns pour identifier les adresses
        address_patterns = [
            r"\b(?:rue|avenue|boulevard|bd|impasse|place|pl|chemin|all√©e)\s+",
            r"\b\d{5}\b",  # Code postal
            r"\b\d{1,4}(?:bis|ter)?\s+(?:rue|avenue|boulevard|impasse)",
        ]
        
        # Patterns pour identifier les villes
        city_patterns = [
            r"\b(?:paris|marseille|lyon|toulouse|nice|nantes|montpellier|strasbourg|bordeaux|lille|rennes|reims|le havre|saint-√©tienne|toulon|grenoble|dijon|angers|n√Æmes|villeurbanne|saint-denis|le mans|aix-en-provence|clermont-ferrand|brest|limoges|tours|amiens|perpignan|metz|besan√ßon|boulogne-billancourt|orl√©ans|mulhouse|rouen|caen|nancy|saint-paul|argenteuil|montreuil|roubaix|tourcoing|nanterre|avignon|cr√©teil|dunkerque|poitiers|asni√®res-sur-seine|versailles|courbevoie|vitry-sur-seine|colombes|pau|aulnay-sous-bois|rueil-malmaison|saint-pierre|antibes|saint-maur-des-foss√©s|cannes|boulogne-sur-mer|noum√©a|calais|drancy|cergy|saint-nazaire|colmar|issy-les-moulineaux|noisy-le-grand|√©vry|villeneuve-d'ascq|la rochelle|antony|troyes|pessac|ivry-sur-seine|clichy|chamb√©ry|lorient|montauban|niort|s√®te|vincennes|saint-ouen|la seyne-sur-mer|villejuif|saint-andr√©|clichy-sous-bois|√©pinay-sur-seine|meaux|merignac|valence|saint-priest|noisy-le-sec|pantin|v√©nissieux|caluire-et-cuire|bourges|la courneuve|cholet|sartrouville|mantes-la-jolie|bobigny)\b",
            r"\b(?:paris\s+\d{1,2})\b"  # Arrondissements parisiens
        ]
        
        import re
        
        # Classification des adresses et entit√©s sp√©cialis√©es
        if entity["label"] in ["LOC", "ORG"] and entity["source"] == "NER":
            # D'abord v√©rifier si c'est un CHU/Centre Hospitalier avant de classer comme ville
            chu_patterns = [
                r"\bCHU\s+(?:de\s+)?[A-Z√â][\w''\-]+(?:\s+[A-Z√â][\w''\-]+){0,2}\b",
                r"\b(?:Centre\s+Hospitalier|CHR)\s+(?:de\s+)?[A-Z√â][\w''\-]+(?:\s+[A-Z√â][\w''\-]+){0,2}\b"
            ]
            
            for chu_pattern in chu_patterns:
                if re.search(chu_pattern, text, re.IGNORECASE):
                    entity["label"] = "ORG_CHU_CH"
                    entity["category"] = "ORG_CHU_CH"
                    entity["source"] = "ENHANCED"
                    logging.debug(f"üè• CHU detected: '{text}' ‚Üí ORG_CHU_CH")
                    return entity
            
            # V√©rifier si c'est une adresse
            is_address = any(re.search(pattern, text.lower()) for pattern in address_patterns)
            if is_address:
                if re.search(r"\b\d{5}\b", text):
                    entity["label"] = "ADDR_FULL"
                    entity["category"] = "ADDR_FULL"
                else:
                    entity["label"] = "ADDR_STREET"
                    entity["category"] = "ADDR_STREET"
                entity["source"] = "ENHANCED"
                logging.debug(f"üè† Address detected: '{text}' ‚Üí {entity['label']}")
                return entity
            
            # V√©rifier si c'est une ville (seulement si pas CHU)
            is_city = any(re.search(pattern, text.lower()) for pattern in city_patterns)
            if is_city:
                entity["label"] = "LOC_CITY"
                entity["category"] = "LOC_CITY"
                entity["source"] = "ENHANCED"
                logging.debug(f"üåç City detected: '{text}' ‚Üí LOC_CITY")
                return entity
        
        # Expansion d'√©tablissement UNIQUEMENT si l'ancre est pr√©sente dans le contexte proche
        if entity["label"] == "LOC" and entity["source"] == "NER":
            for etab_type, keywords in etab_patterns.items():
                # V√©rifier si un mot-cl√© d'√©tablissement est dans le contexte ET proche (¬±30 caract√®res)
                close_context = full_text[max(0, start - 30):min(len(full_text), end + 30)].lower()
                if any(keyword in close_context for keyword in keywords):
                    # Heuristiques d'√©tablissement am√©lior√©es
                    if _is_establishment_name(text, close_context):
                        logging.debug(f"üè¢ LOC‚ÜíETAB: '{text}' reclass√© comme {etab_type}")
                        entity["label"] = etab_type
                        entity["category"] = etab_type
                        entity["source"] = "ENHANCED"
                        break
            
            # Heuristique ETAB_GENERIC pour noms po√©tiques sans gazetteer
            if entity["label"] == "LOC":  # Si pas encore reclass√©
                # Chercher des verbes d'√©tablissement dans le contexte ¬±50
                establishment_context_verbs = [
                    "accueille", "h√©berge", "admet", "suit", "accompagne", 
                    "oriente", "inscrit", "log√©", "plac√©", "admis", "suivi", "h√©berg√©"
                ]
                extended_context = full_text[max(0, start - 50):min(len(full_text), end + 50)].lower()
                if any(verb in extended_context for verb in establishment_context_verbs):
                    if _is_establishment_name(text, extended_context):
                        logging.debug(f"üè¢ LOC‚ÜíETAB_GENERIC: '{text}' reclass√© comme √©tablissement g√©n√©rique")
                        entity["label"] = "ETAB_GENERIC"
                        entity["category"] = "ETAB_GENERIC"
                        entity["source"] = "ENHANCED"

        # Reclassifier les personnes mal √©tiquet√©es comme ORG
        if entity["label"] == "ORG" and entity["source"] == "NER":
            # Patterns de noms de personnes
            name_patterns = [
                r"\b[A-Z][a-z]+ [A-Z][A-Z]+\b",  # Pr√©nom NOM
                r"\bM\. [A-Z][A-Z]+\b",          # M. NOM
                r"\bMme [A-Z][A-Z]+\b",          # Mme NOM
                r"\bDr\. [A-Z][A-Z]+\b"         # Dr. NOM
            ]
            
            for pattern in name_patterns:
                if re.search(pattern, text):
                    logging.debug(f"üë§ ORG‚ÜíPER: '{text}' reclass√© comme personne")
                    entity["label"] = "PER"
                    entity["source"] = "ENHANCED"
                    break
            else:
                # Si c'est une vraie entreprise, cr√©er cat√©gorie sp√©cialis√©e
                if any(word in text.lower() for word in ["carrefour", "auchan", "leclerc", "casino"]):
                    entity["category"] = "ORG_ENTREPRISE_PRIV"
                    entity["source"] = "ENHANCED"
        
        # Disambiguation PER vs ETAB pour noms propres comme "Jean Piaget"
        if entity["label"] == "PER" and entity["source"] == "NER":
            # Chercher des indicateurs d'√©tablissement dans la m√™me phrase
            sentence_context = full_text[max(0, start - 100):min(len(full_text), end + 100)].lower()
            etab_indicators = ["ime", "√©cole", "coll√®ge", "lyc√©e", "√©tablissement", "institution", "centre"]
            if any(indicator in sentence_context for indicator in etab_indicators):
                logging.debug(f"üè¢ PER‚ÜíETAB_GENERIC: '{text}' reclass√© comme √©tablissement (contexte)")
                entity["label"] = "ETAB_GENERIC"
                entity["category"] = "ETAB_GENERIC"
                entity["source"] = "ENHANCED"
        
        # Extension CHU : si une entit√© contient "CHU" et un nom de ville, √©tendre pour capturer le nom complet
        if entity["label"] in ["ORG", "LOC"] and entity["source"] == "NER":
            # Chercher si l'entit√© actuelle contient "CHU"
            if "chu" in text.lower():
                # Examiner le contexte apr√®s l'entit√© pour une √©ventuelle ville
                context_after = full_text[end:min(len(full_text), end + 50)]
                
                # Chercher une ville imm√©diatement apr√®s (avec espaces possibles)
                import re
                city_match = re.search(r'^\s+([A-Z][a-z]+(?:-[A-Z][a-z]+)*)', context_after)
                if city_match:
                    city_name = city_match.group(1)
                    # V√©rifier que ce n'est pas un mot courant qui suivrait CHU
                    excluded_words = ["de", "du", "des", "le", "la", "les", "et", "ou", "avec", "pour"]
                    if city_name.lower() not in excluded_words:
                        # √âtendre l'entit√© pour inclure la ville
                        extended_text = text + " " + city_name
                        entity["text"] = extended_text
                        entity["end"] = end + len(city_match.group(0))
                        entity["label"] = "ORG_CHU_CH"
                        entity["category"] = "ORG_CHU_CH"
                        entity["source"] = "ENHANCED"
                        logging.debug(f"üè• CHU extended: '{text}' ‚Üí '{extended_text}' (ORG_CHU_CH)")
                        return entity
        
        return entity

    def _get_default_priority(self, entity: Dict) -> float:
        """Retourne une priorit√© par d√©faut pour les cat√©gories non d√©finies"""
        source = entity["source"]
        label = entity["label"]
        
        if source == "ENHANCED":
            return 4.5  # Priorit√© √©lev√©e pour les am√©liorations
        elif source == "GAZETTEER":
            return 5.0
        elif source == "RULES":
            if label in ["EMAIL", "NIR", "PHONE", "DATE", "TIME"]:
                return 4.0
            elif label == "ETAB":
                return 3.5  # Priorit√© moyenne pour ETAB non sp√©cifique
            elif label == "ORG":
                return 3.0  # Priorit√© pour ORG non sp√©cifique
            else:
                return 3.0
        else:  # NER
            if label == "PER":
                return 2.0
            else:
                return 1.0

    def _apply_enhanced_filtering(self, text, entities):
        """Application du filtrage avanc√© avec les nouvelles classes"""
        if not ENHANCED_MODULES_AVAILABLE:
            return entities
        
        # 1. Filtrage par stopwords
        filtered_entities = []
        for entity in entities:
            entity_text = text[entity['start']:entity['end']]
            if not self.stopwords_filter.is_loc_stopword(entity_text):
                filtered_entities.append(entity)
            else:
                logging.debug(f"üö´ Entit√© filtr√©e (stopword): '{entity_text}'")
        
        # 2. Mise √† jour des priorit√©s si la matrice est disponible
        for entity in filtered_entities:
            text_content = text[entity['start']:entity['end']]
            entity_type = entity.get('source', '')
            enhanced_priority = self.priority_matrix.get_priority(entity_type)
            if enhanced_priority is not None:
                entity['priority'] = enhanced_priority
                logging.debug(f"üîÑ Priorit√© mise √† jour: '{text_content}' ‚Üí {enhanced_priority}")
        
        return filtered_entities

    def _contextual_disambiguation(self, full_text, entities):
        """D√©sambigu√Øsation contextuelle pour corriger les classifications √©videntes"""
        if not entities:
            return entities
        
        corrected_entities = []
        corrections_applied = 0
        
        for entity in entities:
            original_entity = dict(entity)  # Copie pour √©viter les modifications
            entity_text = entity.get('text', '')
            original_label = entity.get('label', '')
            source = entity.get('source', '')
            
            # Debug: Log toutes les entit√©s ETAB_GENERIC trouv√©es
            if (original_label.startswith('ETAB_GENERIC') and 
                (source.startswith('NER') or source == 'ENHANCED')):  # Inclure les entit√©s reclass√©es par enhance
                logging.debug(f"üîç Entit√© ETAB_GENERIC d√©tect√©e: '{entity_text}' source={source} label={original_label}")
                
                # V√©rifier si c'est un pattern de personne
                if self._is_person_pattern(entity_text):
                    logging.debug(f"   ‚úì Pattern personne d√©tect√© pour '{entity_text}'")
                    
                    # Analyse du contexte autour de l'entit√© (¬±50 caract√®res)
                    context_start = max(0, entity['start'] - 50)
                    context_end = min(len(full_text), entity['end'] + 50)
                    context = full_text[context_start:context_end].lower()
                    
                    # V√©rifier le contexte
                    if self._has_person_context(context, entity_text):
                        logging.debug(f"   ‚úì Contexte personne confirm√© pour '{entity_text}'")
                        original_entity['label'] = 'PER'
                        original_entity['source'] = 'CONTEXTUAL_PER'
                        original_entity['category'] = 'PER'
                        corrections_applied += 1
                        logging.info(f"üîÑ CORRECTION: '{entity_text}' {original_label} ‚Üí PER (contexte)")
                    else:
                        logging.debug(f"   ‚úó Pas de contexte personne pour '{entity_text}'")
                        logging.debug(f"   Contexte analys√©: '{context[:100]}...'")
                else:
                    logging.debug(f"   ‚úó Pas un pattern personne: '{entity_text}'")
            
            # Correction PER ‚Üí PROFESSION pour les titres
            elif (original_label == 'PER' or source.startswith('NER_PER')):
                context_start = max(0, entity['start'] - 50)
                context_end = min(len(full_text), entity['end'] + 50)
                context = full_text[context_start:context_end].lower()
                
                if self._has_professional_title(context, entity_text):
                    logging.debug(f"üîÑ Correction PER‚ÜíPROFESSION: '{entity_text}' (titre d√©tect√©)")
                    original_entity['label'] = 'PROFESSION'
                    original_entity['source'] = 'CONTEXTUAL_PROFESSION'
                    original_entity['category'] = 'PROFESSION'
                    corrections_applied += 1
            
            corrected_entities.append(original_entity)
        
        if corrections_applied > 0:
            logging.info(f"üéØ D√©sambigu√Øsation contextuelle: {corrections_applied} corrections appliqu√©es")
        else:
            etab_generic_count = len([e for e in entities if e.get('label', '').startswith('ETAB_GENERIC') and (e.get('source', '').startswith('NER') or e.get('source', '') == 'ENHANCED')])
            logging.warning(f"‚ö†Ô∏è D√©sambigu√Øsation contextuelle: 0 correction appliqu√©e sur {etab_generic_count} entit√©s ETAB_GENERIC candidates")
        
        return corrected_entities
    
    def _is_person_pattern(self, text):
        """D√©tecte si le texte suit un pattern de nom de personne"""
        import re
        
        # Nettoyer le texte (supprimer espaces et caract√®res parasites)
        clean_text = text.strip()
        
        # Patterns de noms de personnes avec support des accents
        patterns = [
            r'^[A-Z√Ä-√ø][a-z√†-√ø]+\s+[A-Z√Ä-√ø]{2,}$',             # Pr√©nom NOM (toutes majuscules)
            r'^[A-Z√Ä-√ø][a-z√†-√ø]+\s+[A-Z√Ä-√ø][a-z√†-√ø]+$',       # Pr√©nom Nom (style standard)
            r'^[A-Z√Ä-√ø]\.\s*[A-Z√Ä-√ø][a-z√†-√ø]+$',               # P. Nom
            r'^[A-Z√Ä-√ø][a-z√†-√ø]+\s+[A-Z√Ä-√ø][a-z√†-√ø]+(?:\s+[A-Z√Ä-√ø][a-z√†-√ø]+)?$',  # Pr√©nom Nom MiddleName
        ]
        
        for pattern in patterns:
            if re.match(pattern, clean_text):
                return True
                
        return False
    
    def _has_person_context(self, context, entity_text):
        """V√©rifie si le contexte indique qu'il s'agit d'une personne"""
        entity_lower = entity_text.strip().lower()
        
        # Civilit√©s directes
        person_indicators = [
            f'mme {entity_lower}', f'm. {entity_lower}',
            f'monsieur {entity_lower}', f'madame {entity_lower}',
            f'dr {entity_lower}', f'professeur {entity_lower}',
        ]
        
        # Fonctions apr√®s le nom
        function_indicators = [
            f'{entity_lower}, directeur', f'{entity_lower}, directrice',
            f'{entity_lower}, responsable', f'{entity_lower}, chef',
            f'{entity_lower}, tutrice', f'{entity_lower}, tuteur',
        ]
        
        # √Çge et caract√©ristiques personnelles (patterns plus flexibles)
        age_indicators = [
            ', ans', ' ans,', f'{entity_lower}, 2', f'{entity_lower}, 3',
            f'{entity_lower}, 4', f'{entity_lower}, 5', f'{entity_lower}, 6'
        ]
        
        # V√©rifier tous les indicateurs
        all_indicators = person_indicators + function_indicators + age_indicators
        
        for indicator in all_indicators:
            if indicator in context:
                logging.debug(f"üîç Indicateur personne trouv√©: '{indicator}' pour '{entity_text}'")
                return True
        
        # Debug: afficher le contexte si aucun indicateur trouv√©
        logging.debug(f"üîç Pas d'indicateur personne pour '{entity_text}' dans: '{context[:100]}...'")
        return False
    
    def _has_professional_title(self, context, entity_text):
        """V√©rifie si le contexte indique une profession"""
        # Titres professionnels avant le nom
        titles = ['dr ', 'docteur ', 'professeur ', 'pr ']
        
        for title in titles:
            if f'{title}{entity_text.lower()}' in context:
                return True
        
        return False
    
    def resolve_conflicts(self, ner_entities: List[Dict], rules_entities: List[Dict], gazetteer_entities: List[Dict] = None, full_text: str = "") -> List[Dict]:
        """R√©sout les conflits entre NER, r√®gles et gazetteers avec filtrage avanc√© et d√©sambigu√Øsation"""
        all_entities = ner_entities + rules_entities
        if gazetteer_entities:
            all_entities.extend(gazetteer_entities)
        
        # √âtape 1 : Appliquer le filtrage avanc√© si disponible  
        if ENHANCED_MODULES_AVAILABLE and full_text:
            all_entities = self._apply_enhanced_filtering(full_text, all_entities)
        
        # √âtape 2 : Filtrer les entit√©s invalides et am√©liorer la classification
        valid_entities = []
        for entity in all_entities:
            if self._is_valid_entity(entity):
                # Am√©liorer la classification
                enhanced_entity = self._enhance_entity_classification(entity, full_text)
                valid_entities.append(enhanced_entity)
        
        # √âtape 3 : D√©sambigu√Øsation contextuelle (APR√àS l'am√©lioration pour corriger les erreurs)
        if full_text:
            valid_entities = self._contextual_disambiguation(full_text, valid_entities)
        
        resolved = []
        
        # Trier par position
        valid_entities.sort(key=lambda x: x["start"])
        
        for current in valid_entities:
            # V√©rifier les conflits avec les entit√©s d√©j√† r√©solues
            conflicts = []
            for resolved_entity in resolved:
                overlap = self._calculate_overlap(current, resolved_entity)
                if overlap > 0.3:  # Seuil de conflit
                    conflicts.append(resolved_entity)
            
            if not conflicts:
                # Pas de conflit, ajouter directement
                resolved.append(current)
            else:
                # R√©soudre le conflit par priorit√©
                current_priority = self._get_priority(current)
                
                should_replace = True
                for conflict in conflicts:
                    conflict_priority = self._get_priority(conflict)
                    if conflict_priority >= current_priority:
                        should_replace = False
                        break
                
                if should_replace:
                    # Retirer les entit√©s conflictuelles moins prioritaires
                    for conflict in conflicts:
                        resolved.remove(conflict)
                    resolved.append(current)
                    
                    logging.debug(f"üîÑ Conflit r√©solu: '{current['text']}' ({self._get_priority_key(current)}) remplace {len(conflicts)} entit√©(s)")
        
        # Statistiques
        stats = {}
        for entity in resolved:
            key = self._get_priority_key(entity)
            stats[key] = stats.get(key, 0) + 1
        
        logging.info(f"‚öñÔ∏è Conflits r√©solus: {len(resolved)} entit√©s finales {stats}")
        return resolved


def pseudonymize_text(text: str, entities: List[Dict], store: PseudonymStore) -> str:
    """Applique la pseudonymisation sur le texte"""
    entities_sorted = sorted(entities, key=lambda x: x["start"], reverse=True)
    
    result = text
    replacements = 0
    
    for entity in entities_sorted:
        start, end = entity["start"], entity["end"]
        original = entity["text"]
        label = entity["label"]
        category = entity.get("category")  # R√©cup√©rer la cat√©gorie sp√©cifique
        
        actual_text = result[start:end]
        if actual_text == original:
            replacement = store.pseudonymize(original, label, category)
            result = result[:start] + replacement + result[end:]
            replacements += 1
            source_info = f"({entity.get('source', 'UNK')})"
            logging.debug(f"üîÑ {original} ‚Üí {replacement} {source_info}")
        else:
            logging.warning(f"‚ö†Ô∏è Mismatch pos {start}-{end}: '{original}' vs '{actual_text}'")
    
    logging.info(f"‚úÖ {replacements} remplacements effectu√©s")
    return result


def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(description="Pipeline complet NER + R√®gles + Gazetteers")
    parser.add_argument("--input", required=True, help="Fichier d'entr√©e")
    parser.add_argument("--output", required=True, help="Fichier de sortie")
    parser.add_argument("--mapping", help="Fichier de mapping")
    parser.add_argument("--load-mapping", help="Fichier de mapping existant √† charger")
    parser.add_argument("--rules", default="rules/rules.yaml", help="Fichier de r√®gles")
    parser.add_argument("--gazetteers", default="gazetteer", help="Dossier des gazetteers")
    parser.add_argument("--model", default="Jean-Baptiste/camembert-ner", help="Mod√®le NER")
    parser.add_argument("--log-level", choices=["DEBUG", "INFO", "WARNING"], default="INFO")
    parser.add_argument("--depseudonymize", action="store_true", help="Mode d√©pseudonymisation")
    
    args = parser.parse_args()
    
    # Configuration logging
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%H:%M:%S"
    )
    
    if args.depseudonymize:
        logging.info("üîì Mode d√©pseudonymisation")
        if not args.load_mapping:
            logging.error("‚ùå Mode d√©pseudonymisation n√©cessite --load-mapping")
            sys.exit(1)
    else:
        logging.info("üöÄ Pipeline complet NER + R√®gles + Gazetteers")
    
    # Charger le texte
    text = Path(args.input).read_text(encoding="utf-8")
    logging.info(f"üìñ Texte charg√©: {len(text)} caract√®res")
    
    # Normalisation Unicode si les modules avanc√©s sont disponibles
    original_length = len(text)
    if ENHANCED_MODULES_AVAILABLE:
        normalizer = TextNormalizer()
        text = normalizer.normalize_unicode(text)
        if len(text) != original_length:
            logging.info(f"üîÑ Normalisation Unicode: {original_length} ‚Üí {len(text)} caract√®res")
    
    # Mode d√©pseudonymisation
    if args.depseudonymize:
        store = PseudonymStore.load(args.load_mapping)
        result = store.depseudonymize(text)
        logging.info("üîì D√©pseudonymisation termin√©e")
    else:
        # Mode pseudonymisation
        
        # Charger mapping existant si demand√©
        if args.load_mapping:
            store = PseudonymStore.load(args.load_mapping)
        else:
            store = PseudonymStore()
        
        # D√©tection NER
        ner = ChunkedNER(args.model)
        ner_entities = ner.detect_entities(text)
        
        # D√©tection par r√®gles
        rules = RulesEngine(args.rules)
        rules_entities = rules.detect_entities(text)
        
        # D√©tection par gazetteers
        gazetteers = GazetteerEngine(args.gazetteers)
        gazetteer_entities = gazetteers.detect_entities(text)
        
        # R√©solution des conflits
        resolver = ConflictResolver()
        final_entities = resolver.resolve_conflicts(ner_entities, rules_entities, gazetteer_entities, text)
        
        # Pseudonymisation
        result = pseudonymize_text(text, final_entities, store)
    
    # Sauvegarde
    Path(args.output).write_text(result, encoding="utf-8")
    logging.info(f"üíæ R√©sultat sauv√©: {args.output}")
    
    if args.mapping and not args.depseudonymize:
        store.save(args.mapping)
    
    # Statistiques finales
    if not args.depseudonymize:
        logging.info(f"üìä Statistiques: {store.counters}")
    logging.info("üéâ Pipeline termin√©!")


if __name__ == "__main__":
    main()